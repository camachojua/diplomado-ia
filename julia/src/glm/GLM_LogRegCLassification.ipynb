{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM Classification\n",
    "\n",
    "This notebook shows how to do classification with Julia's GLM package.  The data comes from Chap 4 of the ISL book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; using Pkg; \n",
    "#Pkg.add(\"Makie\");\n",
    "#Pkg.add(\"GLMakie\")\n",
    "\n",
    "#Pkg.add(\"CairoMakie\")\n",
    "#Pkg.update()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Plots\n",
    "using StatsPlots\n",
    "using GLM\n",
    "using Statistics\n",
    "using Distributions\n",
    "using Random\n",
    "using MultivariateStats\n",
    "using Makie, GLMakie\n",
    "# using GLMakie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readCSVFile (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#=\n",
    "\t\tfunction readCSVFile( fileName::String, addOnes=\"NO\" )::DataFrame\n",
    "\n",
    "\tRead a CSV file located at string given as argument.\n",
    "\tIf addOnes is omitted the default is \"NO\"\t\n",
    "\tIf addOnes == \"NO\"  return a DataFrame obj that contains the data in the file.\n",
    "\tIf addOnes == \"YES\" return a DataFrame obj that contains ones in first column and\n",
    "\tthe data from the CSV file.\n",
    "=#\n",
    "function readCSVFile( fileName::String, addOnes=\"NO\" )::DataFrame\n",
    "\n",
    "\tif addOnes== \"NO\"\n",
    "\t\tdf = DataFrame(CSV.File( fileName))\n",
    "\t\treturn df\n",
    "\tend\n",
    "\n",
    "\tif addOnes== \"YES\"\n",
    "\t\tdf1 = DataFrame(CSV.File( fileName))\n",
    "\n",
    "\t\tnr = size(df1,1)\n",
    "\t\t#create a df of size (nr, 1) with ones in first column\n",
    "\t\tdf2 = DataFrame(ones(nr, 1), :auto)\n",
    "\t\t# return a df with ones and the content from the CSV file\n",
    "\t\tdf = hcat(df2, df1)\n",
    "\t\t#rename first column as \"x0\" to be consistent with LinReg\n",
    "\t\tn = names(df)\n",
    "\t\trename!(df, n[1] => \"x0\")\n",
    "\t\treturn df\n",
    "\tend\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfConfig (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#=\n",
    "\t\t\tfunction dfConfig( df::DataFrame, cv::Vector )::DataFrame\n",
    "\t\tReceive as arguments a df and a vector. The columns from the df \n",
    "\t\tidentified in the vector are included in the new df object returned.\n",
    "\t\tExample:\n",
    "\n",
    "\t\t\tv = [2,3,4,5,6]\n",
    "\t\t\tdfNew = dfConfig(df, v)\n",
    "\t\t\t# dfNew will have the columns identified in v\n",
    "=#\n",
    "function dfConfig(df::DataFrame, cv::Vector) :: DataFrame\n",
    "\treturn select(df, cv)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df size =10000  nc =5"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CairoMakie.Screen{IMAGE}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#=\n",
    "          function defaultFig41()\n",
    " The ISLR V2 Default credit data set contains 10,000 rows with the following\n",
    "  columns:   \n",
    "            ColNumber default\tstudent\tbalance\tincome\n",
    " \n",
    "  The default column contains \"YES\" / \"NO\" values that need to be translated into 0.0 / 1.0 \n",
    "\n",
    "  Using Makie\n",
    "  \n",
    "  Take a look at https://docs.makie.org/stable/tutorials/getting-started#Getting-started\n",
    "\n",
    "=#\n",
    "\n",
    "using Makie; using CairoMakie\n",
    "\n",
    "function defaultFig41()\n",
    "\n",
    "\n",
    "    df = readCSVFile(\"/hm2/Data/ML_Data/ISL_Data/Default.csv\", \"NO\")\n",
    "    nr = size(df, 1)\n",
    "    nc = size(df, 2)\n",
    "    print(\"df size =\", nr, \"  nc =\", nc)\n",
    "  \n",
    "    # The data columns are [RecordNumber, Default[Y,N], \tStudent[Y,N] Balance, \tIncome]\n",
    "    dfc   = dfConfig( df, [1] )   # get column 1, which contains row number\n",
    "    dfd   = dfConfig( df, [2] )   # get Default\n",
    "    dfs   = dfConfig( df, [3] )   # get Student\n",
    "    dfb   = dfConfig( df, [4] )   # get Balance\n",
    "    dfi   = dfConfig( df, [5] )   # get Income \n",
    "  \n",
    "    ### Replace the elements in Column 1 so that they contain 1.0 as number\n",
    "    nr = size(dfc,1)\n",
    "    dfc = DataFrame(ones(nr, 1), :auto)\n",
    "  \n",
    "    ### Replace the elements of type string that contain \"Yes\" or \"No\" with a number\n",
    "    ### so that logit regression could be computed on this data.\n",
    "    \n",
    "    #replace!(dfd.Default, \"Yes\" => \"1\")\n",
    "    #replace!(dfd.Default, \"No\"  => \"0\")\n",
    "    #dfd[!, :Default] = parse.(Int,dfd[!, :Default])\n",
    "  \n",
    "    xDf = hcat( dfs, dfb, dfi)\n",
    "\n",
    "    X = Matrix(xDf) \n",
    "    Y = Matrix( dfd )\n",
    "\n",
    "    #f = Figure()\n",
    "    #ax = Makie.Axis(f[1, 1], xlabel = \"x label\", ylabel = \"y label\",\n",
    "    #title = \"Title\")\n",
    "    #Makie.scatter!(0:0.5:10, cos, color = :orange)\n",
    "    #f\n",
    "\n",
    "    f1 = Figure()\n",
    "    ax = Makie.Axis(f1[1, 1], title=\"Default Data: Income vs Balance\" , xlabel=\"Balance\", ylabel=\"Income\")\n",
    "    Makie.scatter!(marker=:circle, markersize=10, color=Y[1:10000],   X[1:10000,2], X[1:10000,3] ) # Balance vs Income \n",
    "    f1 \n",
    "    Makie.save(\"/hm2/code/julia/LearnGLM/figs/ISL_Fig41a.png\", f1)\n",
    "   \n",
    "\n",
    "    f3 = Figure()\n",
    "    ax = Makie.Axis(f3[1, 1], title=\"Default Data: Default | Income\" , xlabel=\"Income\", ylabel=\"Default\")\n",
    "    Makie.scatter!(ax, marker=:circle, markersize=3, color=Y[1:10000],   X[1:10000,3], Y[1:10000] ) #Default | Income \n",
    "    # f3\n",
    "    save(\"/hm2/code/julia/LearnGLM/figs/ISL_Fig41c.png\", f3)\n",
    "\n",
    "    f4 = Figure()\n",
    "    ax = Makie.Axis(f4[1, 1], title=\"Default Data: Default | Balance\" , xlabel=\"Balance\", ylabel=\"Default\")\n",
    "    Makie.scatter!(ax, marker=:circle, markersize=3, color=Y[1:10000],   X[1:10000,2], Y[1:10000] ) #Default | Income \n",
    "    # f4\n",
    "    save(\"/hm2/code/julia/LearnGLM/figs/ISL_Fig41d.png\", f4)\n",
    "\n",
    "    f5 = Figure()\n",
    "    ax = Makie.Axis(f5[1, 1], title=\"Default Data: Balance vs Default\" , xlabel=\"Balance\", ylabel=\"Default\")\n",
    "    Makie.scatter!(ax, marker=:circle, markersize=3, color=Y[1:10000],   X[1:10000,2], Y[1:10000] ) # Balance vs Default \n",
    "    # \n",
    "    save(\"/hm2/code/julia/LearnGLM/figs/ISL_Fig42a.png\", f5)\n",
    "end\n",
    "\n",
    "\n",
    "defaultFig41()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df size =10000  nc =5\n",
      " Table 4.1\n",
      "StatsModels.TableRegressionModel{GeneralizedLinearModel{GLM.GlmResp{Vector{Float64}, Binomial{Float64}, LogitLink}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n",
      "\n",
      "Default ~ 1 + Balance\n",
      "\n",
      "Coefficients:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "                    Coef.  Std. Error       z  Pr(>|z|)   Lower 95%    Upper 95%\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "(Intercept)  -10.6513      0.361157    -29.49    <1e-99  -11.3592    -9.94348\n",
      "Balance        0.00549892  0.00022037   24.95    <1e-99    0.005067   0.00593083\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      " Table 4.2\n",
      "StatsModels.TableRegressionModel{GeneralizedLinearModel{GLM.GlmResp{Vector{Float64}, Binomial{Float64}, LogitLink}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n",
      "\n",
      "Default ~ 1 + Student\n",
      "\n",
      "Coefficients:\n",
      "──────────────────────────────────────────────────────────────────────────\n",
      "                 Coef.  Std. Error       z  Pr(>|z|)  Lower 95%  Upper 95%\n",
      "──────────────────────────────────────────────────────────────────────────\n",
      "(Intercept)  -3.50412    0.0706012  -49.63    <1e-99  -3.6425    -3.36575\n",
      "Student       0.404882   0.114939     3.52    0.0004   0.179606   0.630158\n",
      "──────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      " Table 4.2x \n",
      "StatsModels.TableRegressionModel{GeneralizedLinearModel{GLM.GlmResp{Vector{Float64}, Binomial{Float64}, LogitLink}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n",
      "\n",
      "Default ~ 1 + Income\n",
      "\n",
      "Coefficients:\n",
      "───────────────────────────────────────────────────────────────────────────────\n",
      "                   Coef.  Std. Error       z  Pr(>|z|)    Lower 95%   Upper 95%\n",
      "───────────────────────────────────────────────────────────────────────────────\n",
      "(Intercept)  -3.09415     0.146167    -21.17    <1e-98  -3.38063     -2.80767\n",
      "Income       -8.35243e-6  4.20324e-6   -1.99    0.0469  -1.65906e-5  -1.1423e-7\n",
      "───────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      " Table 4.3\n",
      "StatsModels.TableRegressionModel{GeneralizedLinearModel{GLM.GlmResp{Vector{Float64}, Binomial{Float64}, LogitLink}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n",
      "\n",
      "Default ~ 1 + Balance + Income + Student\n",
      "\n",
      "Coefficients:\n",
      "───────────────────────────────────────────────────────────────────────────────────\n",
      "                    Coef.   Std. Error       z  Pr(>|z|)     Lower 95%    Upper 95%\n",
      "───────────────────────────────────────────────────────────────────────────────────\n",
      "(Intercept)  -10.869       0.492256     -22.08    <1e-99  -11.8338      -9.90424\n",
      "Balance        0.00573651  0.000231895   24.74    <1e-99    0.005282     0.00619101\n",
      "Income         3.03345e-6  8.20262e-6     0.37    0.7115   -1.30434e-5   1.91103e-5\n",
      "Student       -0.646776    0.236253      -2.74    0.0062   -1.10982     -0.183729\n",
      "───────────────────────────────────────────────────────────────────────────────────\n",
      " Execute cell\n"
     ]
    }
   ],
   "source": [
    "\n",
    "function defaultDataTable4123()\n",
    "\n",
    "    df = readCSVFile(\"/hm2/Data/ML_Data/ISL_Data/Default.csv\", \"NO\")\n",
    "    nr = size(df, 1)\n",
    "    nc = size(df, 2)\n",
    "    print(\"df size =\", nr, \"  nc =\", nc, \"\\n\")\n",
    "  \n",
    "    dfc   = dfConfig( df, [1] )   # get column 1, which contains row number\n",
    "    dfd   = dfConfig( df, [2] )   # get Default\n",
    "    dfs   = dfConfig( df, [3] )   # get Student\n",
    "    dfb   = dfConfig( df, [4] )   # get Balance\n",
    "    dfi   = dfConfig( df, [5] )   # get Income \n",
    "  \n",
    "    ### Replace the elements in Column 1 so that they contain 1.0 as number\n",
    "    nr = size(dfc,1)\n",
    "    dfc = DataFrame(ones(nr, 1), :auto)\n",
    "  \n",
    "    ### Replace the elements of type string that contain \"Yes\" or \"No\" with a number\n",
    "    ### so that logit regression could be computed on this data.\n",
    "    \n",
    "    #replace!(dfd.Default, \"Yes\" => \"1\")\n",
    "    #replace!(dfd.Default, \"No\"  => \"0\")\n",
    "    #dfd[!, :Default] = parse.(Int,dfd[!, :Default])\n",
    "  \n",
    "    dfy = hcat( dfd, dfs, dfb, dfi)\n",
    "  \n",
    "    ### The formula below produces same results as Table 4.1, pp. 136 from the ISLR2 book.\n",
    "    ### Note that ProbitLink() is NOT used in the \"R\" code of the book\n",
    "    fm = @formula(Default ~ Balance)\n",
    "    logit = glm(fm, dfy, Binomial())\n",
    "    println(\" Table 4.1\")\n",
    "    println(logit)\n",
    "  \n",
    "    ### The formula below produces same results as Table 4.2, pp. 137 from the ISLR2 book.\n",
    "    ### Note1: ProbitLink() is NOT used in the \"R\" code of the book\n",
    "    ### Note2: The \"student\" column contains values \"Yes\" / \"No\". \n",
    "    ###        This is OK when the column is part of the X parameters\n",
    "    ### Note3: The \"default\" column contains values \"Yes\" / \"No\"\n",
    "    ###        Thuis is NOT OK when the column is the independent variable \n",
    "    println(\"\\n\\n Table 4.2\")\n",
    "    fm = @formula(Default ~ Student)\n",
    "    logit = glm(fm, dfy, Binomial())\n",
    "    println(logit)\n",
    " \n",
    "    # I added this formula to get the betas dor Default | income\n",
    "    println(\"\\n\\n Table 4.2x \")\n",
    "    fm = @formula(Default ~ Income)\n",
    "    logit = glm(fm, dfy, Binomial())\n",
    "    println(logit)\n",
    " \n",
    "    ### The formula below produces same results as Table 4.3, pp. 138 from the ISLR2 book.\n",
    "    println(\"\\n\\n Table 4.3\")\n",
    "    fm = @formula(Default ~ Balance + Income + Student)\n",
    "    logit = glm(fm, dfy, Binomial())\n",
    "    println(logit)\n",
    "\n",
    "  end\n",
    "\n",
    "defaultDataTable4123()  \n",
    "println(\" Execute cell\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CairoMakie.Screen{IMAGE}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# p(default | balance) \n",
    "function defaultDataFig42b(b0, b1)\n",
    "\n",
    "    df = readCSVFile(\"/hm2/Data/ML_Data/ISL_Data/Default.csv\", \"NO\")\n",
    "\n",
    "    dfd   = dfConfig( df, [2] )   # get Default\n",
    "    dfb   = dfConfig( df, [4] )   # get Balance\n",
    "\n",
    "    nr = nrow(dfb)\n",
    "\n",
    "    X = Matrix( dfb ) \n",
    "    Y = Matrix( dfd )\n",
    "    ylr = zeros(Float64, nr)\n",
    "\n",
    "    for i = 1:nr \n",
    "        num = exp(b0 + b1*X[i])\n",
    "        ylr[i] = num / (1.0 + num)\n",
    "     end\n",
    "\n",
    "    f = Figure()\n",
    "    ax = Makie.Axis(f[1, 1], title=\"Log Reg       Default | Balance\" , xlabel=\"Balance\", ylabel=\"Default\")\n",
    "    Makie.scatter!(ax, marker=:circle, markersize=3, color=:blue,  X[1:10000], ylr[1:10000] ) # Default | Balance \n",
    "    # lines!(ax, X[1:10000], ylr[1:10000])\n",
    "    \n",
    "    ax = Makie.Axis(f[1, 1])\n",
    "    Makie.scatter!(ax, marker=:circle, markersize=3, color=:red,  X[1:10000], Y[1:10000] ) #  Default | Balance\n",
    "\n",
    "    hideydecorations!(ax, ticks = false)  # Hide y-axis ticks \n",
    "    # f # to display \n",
    "    save(\"/hm2/code/julia/LearnGLM/figs/ISL_Fig42b.png\", f)\n",
    "\n",
    "end\n",
    "\n",
    "beta_0 = -10.65\n",
    "beta_1 = 0.0055\n",
    "\n",
    "defaultDataFig42b( beta_0, beta_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0e-5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# p(default | income)\n",
    "\n",
    "function defaultDataFig42i(b0, b1)\n",
    "\n",
    "    df = readCSVFile(\"/hm2/Data/ML_Data/ISL_Data/Default.csv\", \"NO\")\n",
    "\n",
    "    dfd   = dfConfig( df, [2] )   # get Default\n",
    "    dfi   = dfConfig( df, [5] )   # get Income\n",
    "\n",
    "    nr = nrow(dfd)\n",
    "\n",
    "    X = Matrix( dfi ) \n",
    "    Y = Matrix( dfd )\n",
    "    ylr = zeros(Float64, nr)\n",
    "\n",
    "     for i = 1:nr \n",
    "        num = exp(b0 + b1*X[i])\n",
    "        ylr[i] = num / (1.0 + num)\n",
    "        # ylr[i] = 1.0 - ylr[i] \n",
    "     end\n",
    "\n",
    "    f = Figure()\n",
    "    ax = Makie.Axis(f[1, 1], title=\"Log Reg of    Default | Income \" , xlabel=\"Income\", ylabel=\"Default\")\n",
    "    scatter!(ax, marker=:circle, markersize=3, color=:blue,   X[1:10000], ylr[1:10000] ) # Default | Income \n",
    "    # lines!(ax, X[1:10000], ylr[1:10000])\n",
    "    \n",
    "    ax = Makie.Axis(f[1, 1])\n",
    "    scatter!(ax, marker=:circle, markersize=3, color=:red,   X[1:10000], Y[1:10000] ) # Default | Income \n",
    "    hideydecorations!(ax, ticks = false)  # Hide y-axis ticks \n",
    "    \n",
    "    # f # to display \n",
    "    save(\"/hm2/code/julia/LearnGLM/figs/ISL_Fig42i.jpg\", f)\n",
    "\n",
    "end\n",
    "\n",
    "# for default | income the betas are\n",
    "beta_0 = -3.09 ; beta_1 = 0.00008\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultDataTable44 (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#=\n",
    "          function defaultDataTable44()\n",
    "\n",
    "  Reproduce Table 4.4,, pp. 148 of the ISLR V2 Book.\n",
    "\n",
    "  The method is presented in section 4.4.2 of the ISLR book, where the LDA classifier is \n",
    "  extended to the case of multiple predictors.  X = [ X1, X2, ..., Xp] and it is drawn from \n",
    "  a multi-variate Gaussian distribution, with a class-specific mean vector and a common  covariance \n",
    "  matrix. Each column in X follows its own one0dimentional normal distribution, but there is some\n",
    "  \"correlation\" between each pair of predictors. The function computes LDA on the ISLR V2 Default\n",
    "  Credit data set, that  contains 10,000 rows with the following\n",
    "  columns:   \n",
    "            ColNumber default\tstudent\tbalance\tincome\n",
    " \n",
    "  The default column contains \"YES\" / \"NO\" values that need to be translated into 0.0 / 1.0 \n",
    "\n",
    "  Notes From\n",
    "\n",
    "              https://juliastats.org/MultivariateStats.jl/dev/lda/#MultivariateStats.MulticlassLDA\n",
    "  \n",
    "  LDA is expressed as\n",
    "\n",
    "      w = alfa(Cp + Cn)^-1 (MUp - MUn) =  alpha * (MUp - MUn) / (Cp + Cn)\n",
    "\n",
    "      f(x) = wTx + b\n",
    "\n",
    "  The Julia pkg StatsAPI.PI.fit( LinearDiscriminant, Xp, Xn, covestimator=SimpleCovariance()  )                \n",
    "\n",
    "Following code from \n",
    "\n",
    "https://juliaai.github.io/DataScienceTutorials.jl/isl/lab-4/#lda\n",
    "\n",
    "Fix:\n",
    "\n",
    "  https://stackoverflow.com/questions/68092206/lda-on-julia-using-multivariatestats\n",
    "\n",
    "=#\n",
    "\n",
    "\n",
    "function defaultDataTable44()\n",
    "\n",
    "    df = readCSVFile(\"/hm2/Data/ML_Data/ISL_Data/Default.csv\", \"NO\")\n",
    "    nr = size(df, 1)\n",
    "    nc = size(df, 2)\n",
    "    print(\"df size =\", nr, \"  nc =\", nc, \"\\n\")\n",
    "  \n",
    "    dfc   = dfConfig( df, [1] )   # get column 1, which contains row number\n",
    "    dfd   = dfConfig( df, [2] )   # get Default\n",
    "    dfs   = dfConfig( df, [3] )   # get Student\n",
    "    dfb   = dfConfig( df, [4] )   # get Balance\n",
    "    dfi   = dfConfig( df, [5] )   # get Income \n",
    "  \n",
    "    ### Replace the elements in Column 1 so that they contain 1.0 as number\n",
    "    nr = size(dfc,1)\n",
    "    dfc = DataFrame(ones(nr, 1), :auto)\n",
    "  \n",
    "    ### Replace the elements of type string that contain \"Yes\" or \"No\" with a number\n",
    "    ### so that logit regression could be computed on this data.\n",
    "    \n",
    "    #replace!(dfd.Default, \"Yes\" => \"1\")\n",
    "    #replace!(dfd.Default, \"No\"  => \"0\")\n",
    "    #dfd[!, :Default] = parse.(Int,dfd[!, :Default])\n",
    "    \n",
    "    println(\"Table 4.4\")\n",
    "      \n",
    "    # describe does not work anymore \n",
    "    # print(df)\n",
    "    \n",
    "    xDf = hcat( dfs, dfb, dfi)\n",
    "  \n",
    "    X = Matrix(xDf) \n",
    "    Y = Matrix( dfd )\n",
    "  \n",
    "    #lda = fit(MulticlassLDA, X, Y; outdim=2)\n",
    "    #Ylda = predict(lda, X)\n",
    "  \n",
    "    lda = fit(MulticlassLDA, xDf, dfd, outdim=2)\n",
    "  \n",
    "  \n",
    "  \n",
    "    ##  Defined globally. The @load macro is defined by MLJ\n",
    "    ##  BayesianLDA = @load BayesianLDA pkg=MultivariateStats)\n",
    "    #BayesianLDA = @load BayesianLDA pkg=MultivariateStats\n",
    "    #classif = machine(BayesianLDA(), X, yc)\n",
    "    ## as of 2022_1207 a problem exists here\n",
    "    #fit!(classif)\n",
    "    print(\"defaultDataTable44() completed execution\")\n",
    "    \n",
    "    end\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is DoLab4_Part1\n",
      "Names[\"Year\", \"Lag1\", \"Lag2\", \"Lag3\", \"Lag4\", \"Lag5\", \"Volume\", \"Direction\"]\n",
      "Size=(1250, 8)\n",
      "\u001b[1m8×7 DataFrame\u001b[0m\n",
      "\u001b[1m Row \u001b[0m│\u001b[1m variable  \u001b[0m\u001b[1m mean         \u001b[0m\u001b[1m min        \u001b[0m\u001b[1m median     \u001b[0m\u001b[1m max        \u001b[0m\u001b[1m nmissing \u001b[0m\u001b[1m eltype   \u001b[0m\n",
      "     │\u001b[90m Symbol    \u001b[0m\u001b[90m Float64      \u001b[0m\u001b[90m Real       \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Real       \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m DataType \u001b[0m\n",
      "─────┼─────────────────────────────────────────────────────────────────────────────────\n",
      "   1 │ Year       2003.02       2001        2003.0      2005               0  Int64\n",
      "   2 │ Lag1          0.0038344    -4.922       0.039       5.733           0  Float64\n",
      "   3 │ Lag2          0.0039192    -4.922       0.039       5.733           0  Float64\n",
      "   4 │ Lag3          0.001716     -4.922       0.0385      5.733           0  Float64\n",
      "   5 │ Lag4          0.001636     -4.922       0.0385      5.733           0  Float64\n",
      "   6 │ Lag5          0.0056096    -4.922       0.0385      5.733           0  Float64\n",
      "   7 │ Volume        1.4783        0.35607     1.42295     3.15247         0  Float64\n",
      "   8 │ Direction     0.5184        0           1.0         1               0  Int64\n",
      "StatsModels.TableRegressionModel{GeneralizedLinearModel{GLM.GlmResp{Vector{Float64}, Binomial{Float64}, LogitLink}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n",
      "\n",
      "Direction ~ 1 + Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume\n",
      "\n",
      "Coefficients:\n",
      "────────────────────────────────────────────────────────────────────────────\n",
      "                   Coef.  Std. Error      z  Pr(>|z|)   Lower 95%  Upper 95%\n",
      "────────────────────────────────────────────────────────────────────────────\n",
      "(Intercept)  -0.126        0.240736   -0.52    0.6007  -0.597834   0.345833\n",
      "Lag1         -0.0730737    0.0501674  -1.46    0.1452  -0.1714     0.0252525\n",
      "Lag2         -0.0423013    0.0500861  -0.84    0.3983  -0.140468   0.0558655\n",
      "Lag3          0.0110851    0.0499385   0.22    0.8243  -0.0867926  0.108963\n",
      "Lag4          0.00935894   0.0499741   0.19    0.8514  -0.0885886  0.107306\n",
      "Lag5          0.0103131    0.0495115   0.21    0.8350  -0.0867276  0.107354\n",
      "Volume        0.135441     0.15836     0.86    0.3924  -0.174939   0.44582\n",
      "────────────────────────────────────────────────────────────────────────────\n",
      "cf  = [0.4687506051561676, -0.018150224281176124, -0.010501402874921363, 0.0027747134736188905, 0.0023347687426642414, 0.0025700290526764276, 0.033644714207829954]\n",
      "\n",
      "r   = [0.46875061, -0.01815022, -0.0105014, 0.00277471, 0.00233477, 0.00257003, 0.03364471]\n",
      "\n",
      "Predict=\n",
      "Union{Missing, Float64}[0.5070644034684344, 0.48159331872736677, 0.48129328303166824, 0.5152084539796542, 0.5107959019570415, 0.5069723226667404, 0.49276668930321066, 0.5092356552794334, 0.5175927665328915, 0.48896700452088376, 0.49661057964131655, 0.5197433334004897, 0.5182605296002605, 0.49646148549551355, 0.4866214990340369, 0.5153352055017353, 0.5054310587691341, 0.531823303053418, 0.5166671648433896, 0.4983542275326234, 0.5228918450840354, 0.5220706617611328, 0.5339268247873087, 0.49216772240643814, 0.5008847631408234, 0.5190593587262974, 0.49205132115978, 0.537259556709182, 0.5570582111108268, 0.5573253255185973, 0.5305382240571441, 0.5082675619056674, 0.4715657892075875, 0.4950151550535987, 0.5470462058027407, 0.5259861736294964, 0.5202399798093175, 0.490320912878423, 0.47620522533280024, 0.4852229723229283, 0.49803638774119363, 0.5534778650054415, 0.6191720009606417, 0.5282811076406676, 0.5299110542492218, 0.5152775427928017, 0.5353248737509774, 0.49465325679299693, 0.5248094405907981, 0.5736786745636513, 0.5453322054722642, 0.4767353001446391, 0.45335000145456666, 0.45468182899227644, 0.5378363474181274, 0.5591889752489987, 0.49922933429989746, 0.521849056081833, 0.5865204750257765, 0.5559617270771773, 0.42886093119567903, 0.48972955665777335, 0.5133565638492676, 0.46040679910986576, 0.496415166382835, 0.48492910390586075, 0.4973199821796766, 0.5014658828572983, 0.45401163502588343, 0.4611522482277462, 0.5284831999795109, 0.5541778729936917, 0.5580342119052242, 0.49015603471076125, 0.4796732931423458, 0.47102184673887354, 0.5027188191484403, 0.4962016664197816, 0.5021374753355354, 0.5401635701246932, 0.497215992647555, 0.4895110006461437, 0.5092369632129906, 0.5157910550155137, 0.5121757920990151, 0.5109990897123391, 0.49924826055829974, 0.49798295012276383, 0.46283298969888576, 0.4783238581384694, 0.5077201537711529, 0.4836473706494102, 0.5076633339639404, 0.5436880570579831, 0.5200161886705135, 0.5139323318882685, 0.526407859659941, 0.5378113746542734, 0.5111124802983678, 0.4801813732316436, 0.47962812434421326, 0.4758755942979169, 0.5138976087879272, 0.5123682941766045, 0.5059296681964628, 0.5254600574624017, 0.5096167420579035, 0.5208063577939711, 0.5502037779847301, 0.545332562978542, 0.5125464252866233, 0.4992583859310176, 0.48785658078416244, 0.48967825174287194, 0.5159466945084695, 0.5300726968874983, 0.5198828982344521, 0.5171134737528491, 0.49151814397599086, 0.5169026126910435, 0.491955588716954, 0.4837382404506559, 0.5301597146067487, 0.5613562653759121, 0.5148663700908607, 0.5203122556593116, 0.5257472701065763, 0.4654112791837946, 0.46837888809723377, 0.519537573128666, 0.510707953612421, 0.5171488743153247, 0.5106190292670315, 0.5060117607739711, 0.5382160306758801, 0.554838747761491, 0.4959514484173225, 0.4645206883179625, 0.48403599662163543, 0.501249348824323, 0.505005064311982, 0.5038994029896301, 0.500388094697992, 0.5078249259047135, 0.5257264939042726, 0.5093243245120644, 0.5312334205903032, 0.5211395617390899, 0.48385626239639484, 0.4860038957955031, 0.5042644725758729, 0.5235182355016578, 0.5069802812395502, 0.5258372870148728, 0.49984997842437345, 0.5114056408420375, 0.5053337738640085, 0.4938938177497417, 0.47229308891169974, 0.48298552605681505, 0.5408736228086976, 0.5397124279800602, 0.549999553840064, 0.5024053337625775, 0.49400062544711615, 0.5121272345853134, 0.5526653023940701, 0.5746262857298768, 0.513364481964459, 0.6192917080848421, 0.5781155554741532, 0.5584336159093494, 0.5979939473825815, 0.5953988232525372, 0.46287886907046955, 0.4494855411362927, 0.5184034813008577, 0.5093311549012901, 0.4824262609648931, 0.49369893354026867, 0.49965407031179093, 0.4826165388627124, 0.5149543034143979, 0.5199761369615191, 0.5222353829846376, 0.5335467085051556, 0.4743320593832512, 0.47139104398520915, 0.5101080645083784, 0.5197363305518461, 0.5064725465973073, 0.546389413269687, 0.5453028810168036, 0.508363071235231, 0.4686433092858375, 0.5001740884140224, 0.5212218261817526, 0.4927838048871311, 0.49169680203087035, 0.5475728304096283, 0.5730234751938955, 0.5301804424021256, 0.4621605680925586, 0.4670715444354678, 0.48423332816436093, 0.479038335293241, 0.5164886363940103, 0.5263348059851614, 0.5064333070246301, 0.5074455116694842, 0.4833303000434601, 0.4949568365150654, 0.519242739581323, 0.5228956224534199, 0.5020216259818101, 0.5151248331469386, 0.5225178170753884, 0.4661906783509411, 0.48303188931149593, 0.5182634149751209, 0.5601311423144137, 0.5179626993890608, 0.49920433995088115, 0.5219777880319393, 0.49551424215976836, 0.4739616436012551, 0.5019508955072143, 0.5344514824118833, 0.5542950824687329, 0.539459027023258, 0.5130786620643917, 0.5411509215682502, 0.5183971917659976, 0.48450688015255483, 0.4874241425583398, 0.49979268266734245, 0.5332939858981448, 0.5326021771398226, 0.4803278837946041, 0.4888532266567873, 0.4804591599161976, 0.48864145811683296, 0.519987671896778, 0.5130065255598157, 0.4925519311061831, 0.49859642878231203, 0.5190660211850471, 0.5297860252357445, 0.5320904557746972, 0.5143637011517487, 0.521591162217977, 0.5316695205016411, 0.5060343112942176, 0.5373105477463063, 0.5120388569713252, 0.516552977424376, 0.5373448866113698, 0.5072533771614787, 0.5045159109251232, 0.506431312759508, 0.5089168706431626, 0.5849769185783233, 0.5465028776098789, 0.47397217899906363, 0.5084531071266667, 0.5689900417774542, 0.5664352584844726, 0.5351936167130076, 0.5203956490711173, 0.4822129849894293, 0.46283902009085953, 0.49961738731253647, 0.5024647864976168, 0.510470310996136, 0.541894715911659, 0.5553995651782724, 0.5115314601389319, 0.5209237701441216, 0.5141022456726942, 0.4674911780945703, 0.4961391911549642, 0.5177842041878171, 0.5265771301774395, 0.48440994888370825, 0.46255466706466936, 0.5182473151062447, 0.5112347033950014, 0.5212488659889027, 0.5178424968293804, 0.49759611058167785, 0.5175348697579536, 0.535756445101477, 0.5230859335489721, 0.4969138264626832, 0.4939481960537562, 0.5045447644855083, 0.5392630843077357, 0.5315497269588021, 0.5131347225249699, 0.5331714097280376, 0.50981065724845, 0.48793421892178235, 0.49439115232555014, 0.5019528256674263, 0.5280347332767599, 0.5386208236414902, 0.5190025955556117, 0.5060933923749287, 0.5005427669906748, 0.5167469931300304, 0.5039935734078245, 0.5483652675062849, 0.5269054789613014, 0.5076563909438754, 0.47858241344275076, 0.4875323682663948, 0.5256290324172542, 0.5119316715328666, 0.5409790429506391, 0.5422183008654826, 0.5298530338021644, 0.5248590654422384, 0.5344140282272136, 0.5422286257745143, 0.5085693295767415, 0.4837553349400771, 0.5055745212645022, 0.5348043289019957, 0.5567480859731913, 0.5391417855733001, 0.4461815248379732, 0.48658102016056015, 0.5586622526653389, 0.4932814563351897, 0.4600731534445116, 0.5020935140426752, 0.5109474614499095, 0.49905880516734136, 0.5241139981086496, 0.5452330742732474, 0.5079697619193578, 0.4802366325231992, 0.5053901606523464, 0.5318208805022843, 0.5262449627298895, 0.5212906808675961, 0.5035649068279142, 0.551398373438884, 0.5424623215187938, 0.48935918174471543, 0.5440150815962458, 0.5335629017583771, 0.502577610535884, 0.5336741846051171, 0.5301270014847957, 0.5238890193372159, 0.5347444553892677, 0.4550157940509863, 0.4756669444649917, 0.5474713468750421, 0.5636276140497947, 0.5671583430328476, 0.5249648327798394, 0.534073441527831, 0.5524787205442558, 0.49569277225266856, 0.5193119057147328, 0.5563687059498444, 0.594268370529664, 0.5295472514723499, 0.407985972443739, 0.48340659067504765, 0.5779796003435383, 0.6242718786177562, 0.5605347327987542, 0.5083371906045655, 0.5567759531194449, 0.559559525070196, 0.5636431376333052, 0.5625944979898685, 0.6523104417811075, 0.6334582890390216, 0.6189851134306763, 0.4614234442167781, 0.4753059161294306, 0.5055955091660266, 0.4175888421321716, 0.48378666735721193, 0.5329683846183044, 0.5865445276669247, 0.6110503443445071, 0.5983768255841465, 0.4906145163233439, 0.4286504016417732, 0.43811457391362746, 0.47530845674253375, 0.531051690230713, 0.5711142006472346, 0.47817997760244996, 0.4499915563741165, 0.5067486493233039, 0.47828554281643193, 0.5258693941861292, 0.5149808147202599, 0.47730570116157356, 0.5375700159651082, 0.5163892015316095, 0.5302239729833441, 0.5551838685714178, 0.522786688142706, 0.49720666995534624, 0.5817012292697213, 0.5215383651670499, 0.5144624365417583, 0.48953963311531573, 0.45978603162331094, 0.49018591477910883, 0.4924192455076213, 0.5626768869987137, 0.5358712287704281, 0.49135208524860563, 0.546861821202341, 0.5432077141850808, 0.5751723074194157, 0.5506651745992194, 0.5232491129371446, 0.5632988067996354, 0.48634889955085153, 0.4577123047003985, 0.5582019589057133, 0.5934934106851969, 0.4730633091553359, 0.5187842360276059, 0.5689024236158071, 0.5816367992630068, 0.5818065191809982, 0.508550229740685, 0.5506363568111884, 0.49872573780721885, 0.41497633780038945, 0.4624866998098187, 0.4529280839285762, 0.5362648899898346, 0.5383806126212974, 0.48875042954685105, 0.49250934138239255, 0.5226150288351628, 0.5332602124665408, 0.5491464463702392, 0.5025037887943019, 0.5068610235694334, 0.5483197128360202, 0.5063355641019501, 0.5238392847963552, 0.49068798179967005, 0.48961124763612746, 0.49758754357054796, 0.5051967565765072, 0.5584540998478748, 0.5637659440109465, 0.5508073467821918, 0.5174235870806826, 0.49708065283921166, 0.470707748719267, 0.47517081130110056, 0.5330454491222997, 0.5393090192964745, 0.49366909625710675, 0.4887251391660403, 0.5089737357893908, 0.5302508569134999, 0.5651727609184798, 0.490800482342144, 0.45983991081081793, 0.5328116602508753, 0.5479824151530284, 0.5499708618459287, 0.5310043526676685, 0.5068629647405003, 0.5389322026394783, 0.5080415562611713, 0.4885314670073605, 0.5174731611511104, 0.5395140117620774, 0.4856203183731281, 0.49649142880893105, 0.5522575233141117, 0.5429657626046362, 0.5136904944759227, 0.48190734605501256, 0.4906307149239889, 0.5060339962346466, 0.5289205105386489, 0.5112112523935999, 0.4931057521266844, 0.44604645040394264, 0.4698834601385612, 0.4872799493142098, 0.5167561893725401, 0.5652243799504141, 0.5041578458626997, 0.49870258870533823, 0.5186975824946891, 0.5069651866406913, 0.5415937078682183, 0.5439749518572026, 0.541025907706284, 0.5539278650519884, 0.5481625624330063, 0.5112066879330669, 0.5538921791155467, 0.5734530610877041, 0.5027209378854237, 0.48773530307258933, 0.546253348231498, 0.5227992189578299, 0.48609965573416475, 0.5375429126715616, 0.5409239354381181, 0.5350381365243412, 0.5333838398111386, 0.5005847106865333, 0.5137726587786827, 0.5407855460558411, 0.5320147368695671, 0.47532550247120825, 0.44744747075518354, 0.4996590644989405, 0.543627363540707, 0.5098647055348274, 0.5303018385340774, 0.5245462551314577, 0.5268358997150541, 0.5056337401821479, 0.48752884960179066, 0.5203003489477958, 0.5474759427842723, 0.5143763395499671, 0.5145397640800631, 0.5067382409648373, 0.5448754839393031, 0.5616520618535912, 0.5165543450307016, 0.4565496965282138, 0.4740531931885204, 0.4683435124257609, 0.48578302810082324, 0.517079203024364, 0.5144327138325976, 0.5009541636715612, 0.5557189380088433, 0.5376020049301048, 0.5064038700004598, 0.5200171669271358, 0.5145100526501394, 0.5587252330378893, 0.5111257734826792, 0.45538856009109036, 0.49329504440529515, 0.5163632007369582, 0.521723147129963, 0.5186936834135211, 0.5393234985763626, 0.5151098961343512, 0.5031972366926265, 0.47340413031833034, 0.4828331676071366, 0.5439580011358216, 0.5068655395006141, 0.4963504180152142, 0.4891673892879827, 0.486821155767243, 0.5399859009731879, 0.5544475414617773, 0.4990068401213582, 0.4917864634942151, 0.5268858906665187, 0.5195669572497826, 0.49953143712715753, 0.5089347427567226, 0.516643770948553, 0.5229457094957686, 0.5441072131420575, 0.49759625113055356, 0.4755938535415671, 0.5089973318518667, 0.5290660704765082, 0.5143579847123253, 0.5173145861786608, 0.5635646229967411, 0.5479378400965066, 0.5062306228189887, 0.4898921623236823, 0.49136165960417827, 0.486436306798655, 0.5008992066398339, 0.5384918242900963, 0.5083052157299778, 0.5072467195778037, 0.5089262635840696, 0.49427534592739053, 0.5085890580192067, 0.5369750669805042, 0.54282545310974, 0.5118102158505539, 0.4841467624106893, 0.504780891277923, 0.5310511579529719, 0.4894474546259754, 0.49416896612855454, 0.5248626431645745, 0.5524498541966729, 0.5456025538213868, 0.5359924264182675, 0.5232583293600523, 0.5233245813664915, 0.5020393417633585, 0.5122822797950707, 0.5371046994270924, 0.502856944860905, 0.49032354024616837, 0.4966447802411937, 0.4955210525509483, 0.49784621007936775, 0.5361327831623003, 0.5516958541993724, 0.5104972186746592, 0.492897873701551, 0.5181176280950401, 0.5401144091322836, 0.5566965236557847, 0.5052156052298044, 0.5192728793925425, 0.514029078587785, 0.4991919463089662, 0.5357961769977547, 0.49023769063709804, 0.49969487887723674, 0.5382843523849947, 0.5281414022505934, 0.5215114191862231, 0.5284009473338152, 0.5174580319169594, 0.5407807924274043, 0.5335053914818318, 0.4936136709409705, 0.48824444864370514, 0.4917051749921539, 0.4886823857334399, 0.514105278780574, 0.5078115934639266, 0.4842920904754861, 0.4927908896380687, 0.49809575636544234, 0.514740197933726, 0.5157582223455925, 0.530540888239177, 0.5119700773924767, 0.499549282469218, 0.49939019027901477, 0.49525221488383636, 0.4857018271468592, 0.4901116209602871, 0.5058196754749794, 0.5168669060017631, 0.5336468152588985, 0.5059667558239092, 0.5203976696270676, 0.5540455628842063, 0.5148142855608097, 0.4980497933872564, 0.5086180263976576, 0.49273997573233924, 0.5055203380056795, 0.5021549094172986, 0.5131167539383392, 0.5453057372668795, 0.5165772378638307, 0.5484570966965617, 0.5491001708389937, 0.5290358332011305, 0.49915428374763543, 0.5230463830871176, 0.49162819938398905, 0.4821113301678521, 0.508659051778171, 0.48849583948887715, 0.5074242068263436, 0.5197171775487299, 0.5234517714522549, 0.502952189363972, 0.4930182571970896, 0.49658815631809033, 0.5235075666973678, 0.516001076551367, 0.5315133883792669, 0.5108775836802224, 0.5086211270248187, 0.549857611542187, 0.5313811663887231, 0.5190616331816059, 0.5136721153439998, 0.48934486943978633, 0.5034379527685744, 0.5278022304180043, 0.5182739975562128, 0.5008697049195627, 0.5190918409883272, 0.5268710826771489, 0.5096020580293471, 0.519663444834245, 0.5258241686560673, 0.51465108917671, 0.49279826344451844, 0.5008071744069503, 0.5298842311315144, 0.5371112257365473, 0.5383144721792316, 0.5047658383509115, 0.5142935074915497, 0.5160527208950246, 0.47866571785706785, 0.4939883030318031, 0.49872283847148496, 0.4857041257275432, 0.5006011566306368, 0.5107849242904883, 0.5280838535687433, 0.5129824172357355, 0.5219949222210967, 0.5045324969168172, 0.5243296754767649, 0.5296849987571168, 0.4948986375543035, 0.49247320242485987, 0.5280927455577314, 0.5179078990884358, 0.5100104125961671, 0.5003858967801733, 0.5134296692267782, 0.5095095651770255, 0.5009841462915068, 0.4905154085604836, 0.4811405164818907, 0.481258816429306, 0.4903144816856514, 0.5028227787530861, 0.5143736154335984, 0.5063756875484651, 0.5033267335782307, 0.5237012514051962, 0.5225799577589809, 0.5417155467399554, 0.5224637741006758, 0.5263885240290216, 0.5107878138503474, 0.5119771285956387, 0.5150564789560034, 0.5212998500285135, 0.519110437070048, 0.525094181265688, 0.5321455090101701, 0.49946426019475454, 0.5307791341028588, 0.5677652308145206, 0.5382188784598676, 0.5203408686147181, 0.5143806025037512, 0.5103107466875805, 0.5399117970068136, 0.5272734008391514, 0.4925094847448235, 0.5028305937968341, 0.5113277418152636, 0.5039640518657178, 0.5196901397841504, 0.5319959570243011, 0.5061949597083492, 0.5132099247075503, 0.5336975619036884, 0.5271656428039962, 0.5231663904791086, 0.5237563926071972, 0.506569357553923, 0.5073684704869186, 0.5194122312435631, 0.5024189878432174, 0.5203722164251985, 0.5197946884139945, 0.5041195863905532, 0.5107175746331223, 0.5240961288304316, 0.5401276688085478, 0.5557667238154052, 0.572215447722163, 0.5012817049196179, 0.5264413712377959, 0.5202541373050777, 0.48667211565171026, 0.5061941299812907, 0.5402763270041366, 0.5567009022689755, 0.5334408887015827, 0.519346729268525, 0.4848006042738017, 0.4934897703916546, 0.49715905489484974, 0.4955190667335299, 0.525863978221726, 0.5162875808222627, 0.5066535449568123, 0.4957989031143958, 0.5149311148713843, 0.5375708453935294, 0.5214228371818141, 0.49722623223076357, 0.5338058702301055, 0.5367804821566448, 0.5187486740753238, 0.5067155888695053, 0.4980102373152153, 0.5478895420995092, 0.5355724895841565, 0.4963051445972819, 0.49797011657790785, 0.5207801086290151, 0.5251943873714952, 0.5563512524666295, 0.5592961032445866, 0.5380251167242521, 0.5064030051408114, 0.5047117628126324, 0.5122497437498069, 0.5310058520413472, 0.5596478537021803, 0.5658649193909674, 0.512596523586898, 0.5069334649299627, 0.5120064287410744, 0.5152642513000696, 0.5389354881480769, 0.5130779480200205, 0.515062717146602, 0.5105877966034865, 0.5014815867392621, 0.5040541771702373, 0.49032053849594365, 0.49635684024427235, 0.5112310591274356, 0.5078722247877292, 0.5162483962113239, 0.50586140741678, 0.521596249029462, 0.5054441990451357, 0.47388881114514353, 0.4898355309364697, 0.5310809080178129, 0.515037653414181, 0.5233487078348492, 0.5129681966271505, 0.49519331034483294, 0.5138788783958769, 0.5151242752145564, 0.5128481589722891, 0.5137881402308007, 0.49718756464691005, 0.5128657695131522, 0.5449084027359762, 0.523868785055681, 0.5114331289658676, 0.5059931259248989, 0.5327930999667342, 0.5233993612104175, 0.5287308783800789, 0.5163004464930311, 0.5231733485573321, 0.5090691105630618, 0.4963014725821018, 0.5058675196468404, 0.5222100207924064, 0.5287830644942011, 0.5303626466958609, 0.5172082516372888, 0.5018635584702947, 0.5397097573163863, 0.5350159754231003, 0.5263440454629729, 0.529499655325125, 0.5014934445579982, 0.507524487635479, 0.5111734017384703, 0.5074699899807522, 0.5063219069627248, 0.5219966546618382, 0.5258156662959473, 0.5459977243482245, 0.5645747773474751, 0.5130600518201344, 0.4775366343101423, 0.5005621401035532, 0.5402787436599868, 0.520359613888922, 0.48237514239266965, 0.48997747711766126, 0.4881447739347954, 0.5084367420187232, 0.5085435089083117, 0.5031666902879864, 0.5113719483645823, 0.4937708914618445, 0.49580913129912885, 0.49436231306264217, 0.5107074002182288, 0.5095698523031664, 0.498012861926008, 0.4845394995891863, 0.49515925887175033, 0.5061457552976013, 0.5134415165103214, 0.5199891860680023, 0.4995027297225625, 0.5054632820552519, 0.504114237736088, 0.52357283407394, 0.510852530167843, 0.5045330488244906, 0.5142482108452355, 0.5078384256768598, 0.5340341565051919, 0.5367387732599814, 0.5091876307540647, 0.517459082697432, 0.5069790105543655, 0.4994938071358532, 0.5241050880965228, 0.49568398283481463, 0.5010052737316295, 0.5194396990218239, 0.5093412432899894, 0.533049328739738, 0.5388946393865665, 0.503395202642289, 0.512469062243213, 0.5326402919925802, 0.5413143363310154, 0.5237500218672836, 0.495884637843255, 0.536469918268027, 0.5350390701802858, 0.519662919060167, 0.5323436241878025, 0.5252764404808485, 0.4973519038337163, 0.4865026633436331, 0.508149846629239, 0.5197239777146052, 0.5199859704685891, 0.5286236426155161, 0.5089957920249273, 0.4883615010497755, 0.5059540060163604, 0.5195006486525585, 0.5277081194281392, 0.5266821235630876, 0.5007233266049921, 0.4934738368348086, 0.5107306978946119, 0.5322113499258795, 0.527174691137909, 0.5097814089131533, 0.5387471594956423, 0.516469774262484, 0.5097080227776634, 0.4996305147098656, 0.478500206349893, 0.5231996462997441, 0.5330563010238901, 0.5057649779809422, 0.512615128050963, 0.5234073598483227, 0.5171843312346333, 0.5450952117899553, 0.5225617899149516, 0.5053458527719663, 0.5120699181592374, 0.5016466626150027, 0.5063915708484239, 0.5218173653404906, 0.5337631942261791, 0.5668538024953976, 0.5246794028839128, 0.49979710491289925, 0.49766516005411165, 0.49716403674203474, 0.5102679578440752, 0.4966214874939118, 0.492311242346645, 0.49769315317367224, 0.49811600373197323, 0.5375696345821656, 0.5559904855195194, 0.5435537011415892, 0.513505500004762, 0.5115648582426442, 0.5113161905896486, 0.5258108119529052, 0.5220062399852788, 0.5297867037712404, 0.5124114095871456, 0.495589318708095, 0.5269164388244469, 0.5516302415169996, 0.5450166138769568, 0.5302852188999443, 0.5133449594294613, 0.5063291802034814, 0.5154138199307516, 0.5298644489918934, 0.5150400908675223, 0.50453554771266, 0.5101053849127059, 0.5259260776526211, 0.5117387774417502, 0.5062085557849546, 0.5199045041956485, 0.5362211820837054, 0.5230223837708068, 0.5017657776984045, 0.502898638826539, 0.5141048322653111, 0.5179866337244662, 0.5389593627409723, 0.5289872017739005, 0.5517520523503655, 0.5227009625780816, 0.4937385624406763, 0.4931796844730477, 0.5307864463496107, 0.5285433450861253, 0.5184927784934958, 0.5252978668581183, 0.5056235588371335, 0.5054826954260685, 0.5286403706005458, 0.5527020741383154, 0.5317922675762302, 0.5260566493360119, 0.5117755174451317, 0.5231322442633383, 0.5472018060875733, 0.5244697545345719, 0.5440357135737439, 0.5360870639137688, 0.5617950277571906, 0.5527717567571343, 0.5235645155088393, 0.5206163980513413, 0.5520614505692013, 0.5229306082090078, 0.5282564229864608, 0.5569112462902819, 0.5416650723130588, 0.5222692202377948, 0.519461450164461, 0.519686864144073, 0.5358909820318403, 0.5321471134218503, 0.5249406863046578, 0.5528197917421462, 0.5778840234829281, 0.5981374220959652, 0.5502467683652023, 0.516994195008818, 0.5555063374292, 0.5225420011901489, 0.5275688962277236, 0.5244219722548527, 0.5409666941612283, 0.5491519525358027, 0.5565136501367848, 0.5398047827538575, 0.5100125639420813, 0.5400476519903393, 0.5257426999116861, 0.5314111049942757, 0.5353073519309327, 0.5227560747219916, 0.5473585164673987, 0.5349877595414916, 0.547926220046911, 0.5615351162153202, 0.5131629247708462, 0.5064827072448043, 0.5186574153998799, 0.5126284060664464, 0.5284958189511494, 0.5253321598063242, 0.5241818152059707, 0.5352611525043642, 0.5169359034577797, 0.5068455389535593, 0.5417037888241274, 0.5205773065913729, 0.517386420229608, 0.535500343527946, 0.5267672360921951, 0.5309162772644281, 0.5297936518927177, 0.5213481538852994, 0.5231769186238586, 0.5239077992950613, 0.5188626867776771, 0.5253685824576646, 0.5202219227967256, 0.5387463590598055, 0.5247119856261286, 0.5339010111480638, 0.53373843940788, 0.5570099249924451, 0.5747520243635774, 0.5331211594271676, 0.5079815115954991, 0.5165046261425779, 0.55451453624193, 0.5266400235128679, 0.510784476833994, 0.5366829635919246, 0.53998598202454, 0.5093487760503064, 0.5082612555876088, 0.5247385311933919, 0.5308414743988582, 0.5370902697424323, 0.5240220345008272, 0.5323146499727751, 0.5321808009530801, 0.5218739031121287, 0.5482814721691296, 0.5267934715895934, 0.5287063628518205, 0.5358257502452708, 0.5226233485743121, 0.5221234933128486, 0.537704680213272, 0.5359099542232547, 0.523960269674416, 0.5271522944634113, 0.5482284346349068, 0.5574077967312369, 0.5422902414351773, 0.519522824557485, 0.5336597382877312, 0.5224624715190626, 0.5300946028471075, 0.5257509675617772, 0.5479228245818495, 0.5435298695128729, 0.5264765663404106, 0.5193935161597527, 0.5165560797637452, 0.5297518524174248, 0.5495331736256617, 0.5239284575991122, 0.5268871011489995, 0.5162799525353977, 0.5299444356653226, 0.5346353141045856, 0.5305141223849432, 0.531674354176742, 0.5156840858268525, 0.52265511973592, 0.5419715522568586, 0.5281869056769032, 0.5296941829599803, 0.5552165567279613, 0.5501446785516388, 0.5410997913029815, 0.55642425309237, 0.5374432427050747, 0.5684611335715276, 0.5797977200882564, 0.5519312272542164, 0.5245038516956739, 0.5325652541726893, 0.5335722381106962, 0.5389293345019673, 0.5250179245599793, 0.5286933775292475, 0.5441778888240327, 0.5700148132481203, 0.5939490368039393, 0.5828343105178051, 0.5309705935381182, 0.5445858307077732, 0.5536303667373705, 0.5635806682642366, 0.5542199083069087, 0.5241219709588545, 0.5216545435397514, 0.5583593492936215, 0.5456857639950792, 0.5683890248882024, 0.5674661028127731, 0.5073846472179029, 0.5339929438101813, 0.5632395982095807, 0.5765404631868907, 0.5323349542210991, 0.5201974348175403, 0.5513307682894866, 0.5465903024781014, 0.5468915123983308, 0.5367680331823252, 0.5340867215761885, 0.5424300668521858, 0.5455509259962131, 0.5312566941785551, 0.5142241575949306, 0.5327334987105444, 0.5591913823960994, 0.5435920592508116, 0.5266418742429267, 0.5328202527658196, 0.5278419231213031, 0.5349681494520208, 0.528823949844303, 0.48947441095678845, 0.5533629703117419, 0.5566643042088029, 0.5591873969578834, 0.5398898300097168, 0.5229674807535809, 0.5528445140093584, 0.5412142827384269, 0.5494910354508733, 0.5494008151487628, 0.5270338904814097, 0.5262318100902855, 0.5374139208337102, 0.5280783922180029, 0.5427332910457177, 0.5650227349284487, 0.5586714167727553, 0.5424539406698763, 0.5312626208902135, 0.5198203434174224, 0.5059415598531951, 0.5391587920285593, 0.5260270438558581, 0.5178444487479695]\n",
      "typeof(dfd) = DataFrame\n",
      "names(dfd) = [\"Direction\"]\n",
      "size(dfd) = (1250, 1)\n",
      "[1, 1, 0, 1, 1]\n",
      "This is the end of DoLab4_Part1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#=\n",
    "            function doLab4_Part1()\n",
    "    Reproduce part of Lab4 from the ISLR_V2 book, which is based on Stock Market Data. \n",
    "    The section reproduced here is 4.7.1, wherein LogReg is done\n",
    "    The data consists of percentage returns for the S&P 500 stock\n",
    "    index over 1, 250 days, from the beginning of 2001 until the end of 2005. For \n",
    "    each date, we have recorded the percentage returns for each of the five previous\n",
    "    trading days, Lag1 through Lag5. We have also recorded Volume (the number of \n",
    "    shares traded on the previous day, in billions), Today (the percentage return\n",
    "    on the date in question) and Direction (whether the market was Up or Down on this\n",
    "    date). Our goal is to predict Direction (a qualitative response) using the other\n",
    "    features\n",
    "\n",
    "  fm = @formula(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume)\n",
    "  logit = glm(fm, dfs, Binomial(), ProbitLink())\n",
    " \n",
    "=#\n",
    "function doLab4_Part1()\n",
    "\n",
    "    println()\n",
    "    println(\"This is DoLab4_Part1\")\n",
    "  \n",
    "    ## /home/juan/Data/ML_Data/ISL_Data/ISLR2_1.3-2/csv/\n",
    "    ## Columns = {RECORD\tYear\tLag1\tLag2\tLag3\tLag4\tLag5\tVolume\tToday\tDirection}\n",
    "    ## nRows = 1250\n",
    "  \n",
    "    df = readCSVFile(\"/hm2/Data/ML_Data/ISL_Data/Smarket.csv\", \"NO\")\n",
    "   \n",
    "    ### The code below shows how to select specific columns from a DataFrame.\n",
    "    dfc   = dfConfig( df, [1] )   # get column 1, which contains row number\n",
    "    dfy   = dfConfig( df, [2] )   # get Year\n",
    "    df1   = dfConfig( df, [3] )   # get Lag1\n",
    "    df2   = dfConfig( df, [4] )   # get Lag2\n",
    "    df3   = dfConfig( df, [5] )   # get Lag3 \n",
    "    df4   = dfConfig( df, [6] )   # get Lag4 \n",
    "    df5   = dfConfig( df, [7] )   # get Lag5 \n",
    "    dfv   = dfConfig( df, [8] )   # get Volume\n",
    "    dft   = dfConfig( df, [9] )   # get Today\n",
    "    dfd   = dfConfig( df, [10] )  # get Direction\n",
    "\n",
    "    ### the code below was used to test code changes for Julia 1.10.1\n",
    "    #df_test = hcat( df1, df5)\n",
    "    #println(\"df_test names \", names( df_test) )\n",
    "    #println(\"df_test size \",  size(df_test))\n",
    "    #println( describe(df_test))\n",
    "    #fm = @formula(Lag1 ~ Lag5)\n",
    "    #logit = lm( fm, df_test)\n",
    "    #println(logit)\n",
    "\n",
    "\n",
    "    ### Replace the elements in Column 1 so that they contain 1.0 as number\n",
    "    #nr = size(1, dfc)\n",
    "    #dfc = DataFrame(ones(nr, 1), :auto)\n",
    "  \n",
    "    ### Replace the elements of type string that contain \"Up\" or \"Down\" in \n",
    "    ### column Direction with a number [0,1] to compute LogReg on the data.\n",
    "    replace!(dfd.Direction, \"Up\" => \"1\")\n",
    "    replace!(dfd.Direction, \"Down\" => \"0\")\n",
    "    dfd[!, :Direction] = parse.(Int,dfd[!, :Direction])\n",
    "  \n",
    "    ### dfs is  Stock Market DataFrame\n",
    "    ### for the calculations in pp. 173 of the ISLRV2 book, it makes no difference\n",
    "    ### if dfc and/or dft are added to dfs, so probably it is more efficient to keep\n",
    "    ### them out of dfs\n",
    "    dfs = hcat( dfc, dfy, df1, df2, df3, df4, df5, dfv, dft, dfd)\n",
    "    dfs = hcat( dfy, df1, df2, df3, df4, df5, dfv, dft, dfd) \n",
    "    dfs = hcat( dfc, dfy, df1, df2, df3, df4, df5, dfv, dfd)\n",
    "    dfs = hcat( dfy, df1, df2, df3, df4, df5, dfv, dfd)\n",
    "\n",
    "    println(\"Names\", names(dfs))\n",
    "    println(\"Size=\", size(dfs) )\n",
    "    println( describe(dfs))\n",
    "    \n",
    "    ## produce same result as with dfs\n",
    "    # dfn = hcat(df1, df2, df3, df4, df5, dfv, dfd)\n",
    "\n",
    "    ### The formula below produces same results as pp. 173 from the ISLR2 book.\n",
    "    ### Note that ProbitLink() is NOT used in the \"R\" code of the book\n",
    "    fm = @formula(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume)\n",
    "    logit = glm(fm, dfs, Binomial())\n",
    "    println(logit)\n",
    "\n",
    "    # summary( logit )\n",
    "\n",
    "    model = lm(@formula(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume), dfs)#  + Lag2 + Lag3 + Lag4 + Lag5 + Volume))\n",
    "\n",
    "\n",
    "    ### FOR PREDICT https://www.machinelearningplus.com/julia/logistic-regression-in-julia-practical-guide-with-examples/\n",
    "    ###  https://www.geeksforgeeks.org/logistic-regression-in-julia/\n",
    "\n",
    "    cf = coef(model)\n",
    "    #r = round.(coef(model); digits=8)\n",
    "    r = round.( cf; digits=8)\n",
    "    println(\"cf  = \", cf, \"\\n\")\n",
    "    println(\"r   = \", r, \"\\n\")\n",
    "\n",
    "    # test_data = DataFrame(dfd[1000:1250])\n",
    "    predict = GLM.predict(model, dfs)\n",
    "    println(\"Predict=\")\n",
    "    println(predict)\n",
    "\n",
    "    ### Adding ProbitLink() to the formula produces results that are different from\n",
    "    ### the results in pp. 173 from the ISLR2 book.  \n",
    "    # logit = glm(fm, dfs, Binomial(), ProbitLink())\n",
    "\n",
    "    @show(typeof(dfd))\n",
    "    @show(names(dfd))\n",
    "    @show(size(dfd))\n",
    "    println(dfd[1:5,1])\n",
    "    # test_data = dfd[1000:1250,1]\n",
    "    # predict = GLM.predict(logit, test_data)\n",
    "    # pr = GLM.predict(fm, dfd)\n",
    "\n",
    "    println(\"This is the end of DoLab4_Part1\")\n",
    "    println()\n",
    "  \n",
    "  end\n",
    "  \n",
    "  doLab4_Part1()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GLM Predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefs\n",
      "coefs=[-0.66666667, 2.5]\n",
      "describe(test_data) = 1×7 DataFrame\n",
      " Row │ variable  mean     min    median   max    nmissing  eltype\n",
      "     │ Symbol    Float64  Int64  Float64  Int64  Int64     DataType\n",
      "─────┼──────────────────────────────────────────────────────────────\n",
      "   1 │ X             4.0      4      4.0      4         0  Int64\n",
      "\u001b[1m1×1 DataFrame\u001b[0m\n",
      "\u001b[1m Row \u001b[0m│\u001b[1m X     \u001b[0m\n",
      "     │\u001b[90m Int64 \u001b[0m\n",
      "─────┼───────\n",
      "   1 │     4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 9.33333333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Try to do prediction following the example from the GLM Documentation \n",
    "data = DataFrame(X=[1,2,3], y=[2,4,7]);\n",
    "mdl = lm(@formula(y ~ X), data);\n",
    "\n",
    "println(\"coefs\")\n",
    "cf = round.(coef(mdl); digits=8)\n",
    "print(\"coefs=\", cf, \"\\n\" )\n",
    "\n",
    "round(r2(mdl); digits=8)\n",
    "# round(aic(mdl); digits=8)\n",
    "\n",
    "test_data = DataFrame(X=[4]);\n",
    "@show(describe(test_data))\n",
    "println(test_data)\n",
    "round.(predict(mdl, test_data); digits=8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
