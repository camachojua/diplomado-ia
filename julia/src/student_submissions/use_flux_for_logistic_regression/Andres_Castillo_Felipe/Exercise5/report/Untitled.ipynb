{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba21e95c-e040-4f84-84e2-a6558418e944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "displayCorrelation (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Pkg\n",
    "#Pkg.add(\"Flux\")\n",
    "\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Flux\n",
    "using Statistics\n",
    "using Random\n",
    "\n",
    "#Se utilizan algunas funciones definidas en el ejercicio 1\n",
    "include(\"./../src/exercise1_code.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a30e3ae-fd27-452d-a36a-4e89822609fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoding (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function OneHotEncoding(column)\n",
    "    variables = unique(column)\n",
    "    n = 0\n",
    "    for var in variables\n",
    "        replace!(column, var => \"$(n)\")\n",
    "        n+=1\n",
    "    end\n",
    "    column = parse.(Int, column)\n",
    "    return column\n",
    "end     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "846d45c0-fe2a-49f6-91c2-c727e69ea511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 14)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM_data = CSV.read(\"./../dat/Churn_Modelling.csv\", DataFrame)\n",
    "rows, cols = dataShape(CM_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55da16be-436b-4592-b1ce-ed5f235a2e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables categoricas string codificadas\n",
    "CM_data.Gender = OneHotEncoding(CM_data.Gender)\n",
    "CM_data.Geography = OneHotEncoding(CM_data.Geography)\n",
    "\n",
    "# Variables cuantitativas normalizadas\n",
    "CM_data.Balance = Flux.normalise(CM_data.Balance)\n",
    "CM_data.EstimatedSalary = Flux.normalise(CM_data.EstimatedSalary)\n",
    "\n",
    "#Variables predictoras y variable objetivo\n",
    "X = select(CM_data, Not([\"RowNumber\", \"CustomerId\", \"Surname\", \"Exited\"]))\n",
    "y = Float32.(CM_data.Exited);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "693f220b-633f-4fd9-a293-9475372344a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase 0: 5567\n",
      "Clase 1: 1433\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(18)\n",
    "\n",
    "# rows es el número total de registros\n",
    "# Muestra aleatoria de indices para los datos de entrenamiento (70% de los datos)\n",
    "idx_train = sample(1:rows, Int(round(0.7*rows)), replace = false)\n",
    "# Indices restantes para los datos de prueba\n",
    "idx_test = Not(idx_train)\n",
    "\n",
    "# Datos entrenamiento\n",
    "X_train = Float32.(Matrix(X[idx_train,:])')\n",
    "y_train = y[idx_train]\n",
    "\n",
    "class0_idx = findall(y_train .== 0.0)\n",
    "class1_idx = findall(y_train .== 1.0)\n",
    "println(\"Clase 0: \", length(class0_idx))\n",
    "println(\"Clase 1: \", length(class1_idx))\n",
    "\n",
    "oversampled_idx = vcat(class0_idx, sample(class1_idx, length(class0_idx), replace=true))\n",
    "X_train =X_train[:,oversampled_idx]\n",
    "y_train = y_train[oversampled_idx]\n",
    "\n",
    "# Datos prueba\n",
    "X_test = Float32.(Matrix(X[idx_test,:])')\n",
    "y_test = y[idx_test];\n",
    "# #y_test = Flux.onehotbatch(y[idx_test], [0,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46687f09-f8b1-44b0-96b5-991c1323174e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Accuracy: 0.5\n",
      "Epoch 2 - Accuracy: 0.5\n",
      "Epoch 3 - Accuracy: 0.5\n",
      "Epoch 4 - Accuracy: 0.5\n",
      "Epoch 5 - Accuracy: 0.5\n",
      "Epoch 6 - Accuracy: 0.5\n",
      "Epoch 7 - Accuracy: 0.5\n",
      "Epoch 8 - Accuracy: 0.5\n",
      "Epoch 9 - Accuracy: 0.5\n",
      "Epoch 10 - Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Definir modelo: una sola salida con sigmoide\n",
    "#model = Chain(Dense(10 => 5, relu), Dense(5 => 5, relu), Dense(5 => 1))  # σ es la función sigmoide\n",
    "model = Chain(Dense(10 => 1, σ))\n",
    "function loss(model, X, y)\n",
    "    ŷ = model(X)[1,:]  # Aplanar salida para calcular la pérdida\n",
    "    Flux.logitbinarycrossentropy(ŷ, y)\n",
    "end\n",
    "\n",
    "# Métrica de precisión\n",
    "accuracy(model, X, y) = mean((model(X)[:, 1] .> 0.5) .== y)\n",
    "\n",
    "# Optimizador\n",
    "optimizer = Flux.setup(Adam(0.1), model)\n",
    "\n",
    "# Entrenamiento\n",
    "data = [(X_train, y_train)]\n",
    "for epoch in 1:10\n",
    "    Flux.train!(loss, model, data, optimizer)\n",
    "    println(\"Epoch $epoch - Accuracy: \", accuracy(model, X_train, y_train))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae7cd01a-64ad-40b5-afeb-4a2171b5ccdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×11134 Matrix{Float32}:\n",
       " 3.71157f-35  7.0375f-29  1.13027f-32  …  3.27187f-32  5.22469f-34"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eff59c0-3fe2-4f66-a74a-fd025109c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modelo: una red con una capa densa, 10 entradas y 1 salida, cuya función de activación es la sigmoide\n",
    "# model = Chain(Dense(10 => 2), softmax)\n",
    "\n",
    "# #Loss function \n",
    "# function loss(model, X, y)\n",
    "#     ŷ = model(X)\n",
    "#     Flux.crossentropy(ŷ,y)\n",
    "# end\n",
    "\n",
    "# # Accuracy function\n",
    "# accuracy(model, X, y) = mean(Flux.onecold(model(X),[0,1]) .== Flux.onecold(y,[0,1]))\n",
    "\n",
    "# #Optimizador\n",
    "# optimizer = Flux.setup(Adam(0.001), model)\n",
    "\n",
    "# # Entrenamiento\n",
    "# data = [(X_train, y_train)]\n",
    "# for epoch in 1:50\n",
    "#     Flux.train!(loss, model, data, optimizer)\n",
    "#     println(\"Epoch $epoch - Accuracy: \", accuracy(model, X_train, y_train))\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c08d41b-743f-40b9-b40f-ca7774090769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[786.0 636.0 … 676.0 611.0; 1.0 2.0 … 0.0 0.0; … ; 0.0 1.0 … 0.0 1.0; 0.01459546 -0.80994624 … -0.085365735 -0.5886526], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(X_train, y_train)][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df4c3d7d-2283-460b-a8b3-3431552e9cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zip(Float32[786.0 636.0 … 676.0 611.0; 1.0 2.0 … 0.0 0.0; … ; 0.0 1.0 … 0.0 1.0; 0.01459546 -0.80994624 … -0.085365735 -0.5886526], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5c85781-8153-4092-a242-47a21c27f960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "train!(loss, model, data, opt_state)\n",
       "\\end{verbatim}\n",
       "Uses a \\texttt{loss} function and training \\texttt{data} to improve the \\texttt{model}'s parameters according to a particular optimisation rule encoded in \\texttt{opt\\_state}. Iterates through \\texttt{data} once, evaluating for each \\texttt{d in data} either \\texttt{loss(model, d...)} if \\texttt{d isa Tuple}, or else \\texttt{loss(model, d)} for other \\texttt{d}.\n",
       "\n",
       "If \\texttt{model} is an Enzyme.Duplicated and \\texttt{Enzyme.jl} is loaded, gradients will be computed with Enzyme, otherwise they will be computed with Zygote.\n",
       "\n",
       "For example, with these definitions...\n",
       "\n",
       "\\begin{verbatim}\n",
       "data = [(x1, y1), (x2, y2), (x3, y3)]\n",
       "\n",
       "loss3(m, x, y) = norm(m(x) .- y)        # the model is the first argument\n",
       "\n",
       "opt_state = Flux.setup(Adam(), model)   # explicit setup of optimiser momenta\n",
       "\\end{verbatim}\n",
       "...calling \\texttt{Flux.train!(loss3, model, data, opt\\_state)} runs a loop much like this:\n",
       "\n",
       "\\begin{verbatim}\n",
       "for d in data\n",
       "    ∂L∂m = gradient(loss3, model, d...)[1]\n",
       "    update!(opt_state, model, ∂L∂m)\n",
       "end\n",
       "\\end{verbatim}\n",
       "You can also write this loop yourself, if you need more flexibility. For this reason \\texttt{train!} is not highly extensible. It adds only a few features to the loop above:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item Stop with a \\texttt{DomainError} if the loss is infinite or \\texttt{NaN} at any point.\n",
       "\n",
       "\n",
       "\\item Show a progress bar using \\href{https://github.com/JuliaLogging/ProgressLogging.jl}{\\texttt{@withprogress}}.\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{quote}\n",
       "\\textbf{compat}\n",
       "\n",
       "New\n",
       "\n",
       "This method was added in Flux 0.13.9. It has significant changes from the one used by Flux ≤ 0.13:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item It now takes the \\texttt{model} itself, not the result of \\texttt{Flux.params}. (This is to move away from Zygote's \"implicit\" parameter handling, with \\texttt{Grads}.)\n",
       "\n",
       "\n",
       "\\item Instead of \\texttt{loss} being a function which accepts only the data, now it must also accept the \\texttt{model} itself, as the first argument.\n",
       "\n",
       "\n",
       "\\item \\texttt{opt\\_state} should be the result of \\href{@ref}{\\texttt{Flux.setup}}. Using an optimiser such as \\texttt{Adam()} without this step should give you a warning.\n",
       "\n",
       "\n",
       "\\item Callback functions are not supported. (But any code can be included in the above \\texttt{for} loop.)\n",
       "\n",
       "\\end{itemize}\n",
       "\\end{quote}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "train!(loss, Duplicated(model), data, opt_state)\n",
       "\\end{verbatim}\n",
       "This method uses Enzyme.jl instead of Zygote.jl to compute the gradients, but is otherwise the same as \\texttt{train!(loss, model, data, opt\\_state)}.\n",
       "\n",
       "Only available when Enzyme is loaded.\n",
       "\n",
       "\\begin{quote}\n",
       "\\textbf{compat}\n",
       "\n",
       "New\n",
       "\n",
       "This method was added in Flux 0.13.9.\n",
       "\n",
       "\\end{quote}\n"
      ],
      "text/markdown": [
       "```\n",
       "train!(loss, model, data, opt_state)\n",
       "```\n",
       "\n",
       "Uses a `loss` function and training `data` to improve the `model`'s parameters according to a particular optimisation rule encoded in `opt_state`. Iterates through `data` once, evaluating for each `d in data` either `loss(model, d...)` if `d isa Tuple`, or else `loss(model, d)` for other `d`.\n",
       "\n",
       "If `model` is an Enzyme.Duplicated and `Enzyme.jl` is loaded, gradients will be computed with Enzyme, otherwise they will be computed with Zygote.\n",
       "\n",
       "For example, with these definitions...\n",
       "\n",
       "```\n",
       "data = [(x1, y1), (x2, y2), (x3, y3)]\n",
       "\n",
       "loss3(m, x, y) = norm(m(x) .- y)        # the model is the first argument\n",
       "\n",
       "opt_state = Flux.setup(Adam(), model)   # explicit setup of optimiser momenta\n",
       "```\n",
       "\n",
       "...calling `Flux.train!(loss3, model, data, opt_state)` runs a loop much like this:\n",
       "\n",
       "```\n",
       "for d in data\n",
       "    ∂L∂m = gradient(loss3, model, d...)[1]\n",
       "    update!(opt_state, model, ∂L∂m)\n",
       "end\n",
       "```\n",
       "\n",
       "You can also write this loop yourself, if you need more flexibility. For this reason `train!` is not highly extensible. It adds only a few features to the loop above:\n",
       "\n",
       "  * Stop with a `DomainError` if the loss is infinite or `NaN` at any point.\n",
       "  * Show a progress bar using [`@withprogress`](https://github.com/JuliaLogging/ProgressLogging.jl).\n",
       "\n",
       "!!! compat \"New\"\n",
       "    This method was added in Flux 0.13.9. It has significant changes from the one used by Flux ≤ 0.13:\n",
       "\n",
       "      * It now takes the `model` itself, not the result of `Flux.params`. (This is to move away from Zygote's \"implicit\" parameter handling, with `Grads`.)\n",
       "      * Instead of `loss` being a function which accepts only the data, now it must also accept the `model` itself, as the first argument.\n",
       "      * `opt_state` should be the result of [`Flux.setup`](@ref). Using an optimiser such as `Adam()` without this step should give you a warning.\n",
       "      * Callback functions are not supported. (But any code can be included in the above `for` loop.)\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "train!(loss, Duplicated(model), data, opt_state)\n",
       "```\n",
       "\n",
       "This method uses Enzyme.jl instead of Zygote.jl to compute the gradients, but is otherwise the same as `train!(loss, model, data, opt_state)`.\n",
       "\n",
       "Only available when Enzyme is loaded.\n",
       "\n",
       "!!! compat \"New\"\n",
       "    This method was added in Flux 0.13.9.\n",
       "\n"
      ],
      "text/plain": [
       "\u001b[36m  train!(loss, model, data, opt_state)\u001b[39m\n",
       "\n",
       "  Uses a \u001b[36mloss\u001b[39m function and training \u001b[36mdata\u001b[39m to improve the \u001b[36mmodel\u001b[39m's parameters\n",
       "  according to a particular optimisation rule encoded in \u001b[36mopt_state\u001b[39m. Iterates\n",
       "  through \u001b[36mdata\u001b[39m once, evaluating for each \u001b[36md in data\u001b[39m either \u001b[36mloss(model, d...)\u001b[39m if\n",
       "  \u001b[36md isa Tuple\u001b[39m, or else \u001b[36mloss(model, d)\u001b[39m for other \u001b[36md\u001b[39m.\n",
       "\n",
       "  If \u001b[36mmodel\u001b[39m is an Enzyme.Duplicated and \u001b[36mEnzyme.jl\u001b[39m is loaded, gradients will be\n",
       "  computed with Enzyme, otherwise they will be computed with Zygote.\n",
       "\n",
       "  For example, with these definitions...\n",
       "\n",
       "\u001b[36m  data = [(x1, y1), (x2, y2), (x3, y3)]\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  loss3(m, x, y) = norm(m(x) .- y)        # the model is the first argument\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  opt_state = Flux.setup(Adam(), model)   # explicit setup of optimiser momenta\u001b[39m\n",
       "\n",
       "  ...calling \u001b[36mFlux.train!(loss3, model, data, opt_state)\u001b[39m runs a loop much like\n",
       "  this:\n",
       "\n",
       "\u001b[36m  for d in data\u001b[39m\n",
       "\u001b[36m      ∂L∂m = gradient(loss3, model, d...)[1]\u001b[39m\n",
       "\u001b[36m      update!(opt_state, model, ∂L∂m)\u001b[39m\n",
       "\u001b[36m  end\u001b[39m\n",
       "\n",
       "  You can also write this loop yourself, if you need more flexibility. For\n",
       "  this reason \u001b[36mtrain!\u001b[39m is not highly extensible. It adds only a few features to\n",
       "  the loop above:\n",
       "\n",
       "    •  Stop with a \u001b[36mDomainError\u001b[39m if the loss is infinite or \u001b[36mNaN\u001b[39m at any\n",
       "       point.\n",
       "\n",
       "    •  Show a progress bar using \u001b[36m@withprogress\u001b[39m\n",
       "       (https://github.com/JuliaLogging/ProgressLogging.jl).\n",
       "\n",
       "\u001b[39m\u001b[1m  │ \u001b[22m\u001b[39m\u001b[1mNew\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m  This method was added in Flux 0.13.9. It has significant changes\n",
       "\u001b[39m\u001b[1m  │\u001b[22m  from the one used by Flux ≤ 0.13:\n",
       "\u001b[39m\u001b[1m  │\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m    •  It now takes the \u001b[36mmodel\u001b[39m itself, not the result of\n",
       "\u001b[39m\u001b[1m  │\u001b[22m       \u001b[36mFlux.params\u001b[39m. (This is to move away from Zygote's\n",
       "\u001b[39m\u001b[1m  │\u001b[22m       \"implicit\" parameter handling, with \u001b[36mGrads\u001b[39m.)\n",
       "\u001b[39m\u001b[1m  │\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m    •  Instead of \u001b[36mloss\u001b[39m being a function which accepts only the\n",
       "\u001b[39m\u001b[1m  │\u001b[22m       data, now it must also accept the \u001b[36mmodel\u001b[39m itself, as the\n",
       "\u001b[39m\u001b[1m  │\u001b[22m       first argument.\n",
       "\u001b[39m\u001b[1m  │\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m    •  \u001b[36mopt_state\u001b[39m should be the result of \u001b[36mFlux.setup\u001b[39m. Using an\n",
       "\u001b[39m\u001b[1m  │\u001b[22m       optimiser such as \u001b[36mAdam()\u001b[39m without this step should give\n",
       "\u001b[39m\u001b[1m  │\u001b[22m       you a warning.\n",
       "\u001b[39m\u001b[1m  │\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m    •  Callback functions are not supported. (But any code can\n",
       "\u001b[39m\u001b[1m  │\u001b[22m       be included in the above \u001b[36mfor\u001b[39m loop.)\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  train!(loss, Duplicated(model), data, opt_state)\u001b[39m\n",
       "\n",
       "  This method uses Enzyme.jl instead of Zygote.jl to compute the gradients,\n",
       "  but is otherwise the same as \u001b[36mtrain!(loss, model, data, opt_state)\u001b[39m.\n",
       "\n",
       "  Only available when Enzyme is loaded.\n",
       "\n",
       "\u001b[39m\u001b[1m  │ \u001b[22m\u001b[39m\u001b[1mNew\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m  This method was added in Flux 0.13.9."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Flux.train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f507210a-8a46-4311-8b93-6447967266f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b38c591-3c8f-4896-84b6-812e58e49425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b8071-03c1-42d6-a610-7463df504e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(19)\n",
    "\n",
    "# rows es el número total de registros\n",
    "# Muestra aleatoria de indices para los datos de entrenamiento (70% de los datos)\n",
    "idx_train = sample(1:rows, Int(round(0.7*rows)), replace = false)\n",
    "# Indices restantes para los datos de prueba\n",
    "idx_test = Not(idx_train)\n",
    "# Datos entrenamiento\n",
    "X_train = Float32.(Matrix(X[idx_train,:])')\n",
    "y_train = Flux.onehotbatch(y[idx_train], [0,1])\n",
    "# Datos prueba\n",
    "X_test = Float32.(Matrix(X[idx_test,:])')\n",
    "y_test = Flux.onehotbatch(y[idx_test], [0,1]);\n",
    "\n",
    "# Nota: se usa la transpuesta de la matriz para X_train, X_test para obtener una matriz de 10 filas y n columnas. Este formato es necesario.\n",
    "# también se convierten los valores Float64 a Float32, pues así lo sugiere la documentación para tener un mejor rendimiento\n",
    "\n",
    "# Modelo: una red con una capa densa, 10 entradas y 1 salida, cuya función de activación es la sigmoide\n",
    "model = Chain(Dense(10 => 2), sigmoid)\n",
    "\n",
    "# Loss function \n",
    "function loss(model, X, y)\n",
    "    ŷ = model(X)\n",
    "    Flux.binarycrossentropy(ŷ,y)\n",
    "end\n",
    "\n",
    "# Accuracy function\n",
    "accuracy(model, X, y) = mean(Flux.onecold(model(X),[0,1]) .== Flux.onecold(y,[0,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253854c-265d-4af7-a166-51affdfc381c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0febfa-44d9-4d3e-8913-51a8b38d3dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd21b680-e47a-4529-ad63-9e1952783f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_processed_data()\n",
    "    \n",
    "    CM_data = CSV.read(\"./../dat/Churn_Modelling.csv\", DataFrame)\n",
    "    CM_data.Gender = OneHotEncoding(CM_data.Gender)\n",
    "    CM_data.Geography = OneHotEncoding(CM_data.Geography)\n",
    "    CM_data.Balance = Flux.normalise(CM_data.Balance)\n",
    "    CM_data.EstimatedSalary = Flux.normalise(CM_data.EstimatedSalary)\n",
    "    data = select(CM_data, Not([\"RowNumber\", \"CustomerId\", \"Surname\"]))\n",
    "    rows, cols = dataShape(CM_data)\n",
    "\n",
    "    X = select(data, Not(:Exited))\n",
    "    y = data.Exited\n",
    "\n",
    "    idx_train = sample(1:rows, Int(round(0.7*rows)), replace = false)\n",
    "    idx_test = Not(idx_train)\n",
    "    # Datos entrenamiento\n",
    "    X_train = Float32.(Matrix(X[idx_train,:])')\n",
    "    y_train = Flux.onehotbatch(y[idx_train], [0,1])\n",
    "    # Datos prueba\n",
    "    X_test = Float32.(Matrix(X[idx_test,:])')\n",
    "    y_test = Flux.onehotbatch(y[idx_test], [0,1])\n",
    "\n",
    "    ## Repeat the data `args.repeat` times\n",
    "    train_data = (X_train, y_train)\n",
    "    test_data = (X_test,y_test)\n",
    "\n",
    "    return train_data, test_data\n",
    "end\n",
    "\n",
    "accuracy(model, x, y) = mean(Flux.onecold(model(x),[0,1]) .== Flux.onecold(y,[0,1]))\n",
    "\n",
    "function train()\t\n",
    "\n",
    "    ## Load processed data\n",
    "    train_data, test_data = get_processed_data()\n",
    "\n",
    "    ## #Declare model \n",
    "    model = Chain(Dense(10, 2))\n",
    "\t\n",
    "    ## Define loss function to be used in training\n",
    "    ## For numerical stability, we use here logitcrossentropy\n",
    "    loss(m, x, y) = logitcrossentropy(m(x), y)\n",
    "\t\n",
    "    ## Training\n",
    "    ## Gradient descent optimiser\n",
    "    optimiser = Descent()\n",
    "\n",
    "    println(\"Starting training.\")\n",
    "    Flux.train!(loss, model, train_data, optimiser)\n",
    "\t\n",
    "    return model, test_data\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
