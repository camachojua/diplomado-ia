{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "\n",
    "# Pkg.add(\"CSV\")\n",
    "# Pkg.add(\"Tables\")\n",
    "# Pkg.add(\"DataFrames\")\n",
    "# Pkg.add(\"JSON\")\n",
    "# Pkg.add(\"Statistics\")\n",
    "\n",
    "using CSV\n",
    "using Tables\n",
    "using DataFrames\n",
    "using JSON\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large DataSet Staggered Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Master (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function Master(file_path::String, chunk_size::Int)\n",
    "    row_start = 2\n",
    "\n",
    "    threads = []\n",
    "    println(\"starting\")\n",
    "\n",
    "    while true\n",
    "        # Load the current chunk of data\n",
    "        data = CSV.File(file_path; header=true, limit=chunk_size, skipto=row_start)\n",
    "        if Tables.isempty(data)\n",
    "            break\n",
    "        end\n",
    "        t = (row_start + chunk_size-2)/chunk_size\n",
    "        println(\"Launching worker $t\")\n",
    "        # Launch a worker for the current chunk\n",
    "        push!(threads,Threads.@spawn Worker(DataFrame(data),movies))\n",
    "\n",
    "        # Move to the next chunk\n",
    "        row_start += chunk_size\n",
    "    end\n",
    "\n",
    "    # Collect the results from each worker\n",
    "    local_metrics_with_time = [fetch(thread) for thread in threads]\n",
    "\n",
    "    # Aggregate the local metrics from each worker\n",
    "    \n",
    "    # Initialize global metrics dictionary\n",
    "    global_metrics = Dict{String, Dict{String, Float64}}()\n",
    "    worker_times = []\n",
    "\n",
    "    # Aggregate metrics across all chunks\n",
    "    for (local_metric, elapsed_time) in local_metrics_with_time\n",
    "        local_data = JSON.parse(local_metric)  # Parse JSON string to dictionary\n",
    "        push!(worker_times, elapsed_time)\n",
    "\n",
    "        for (genre, metrics) in local_data\n",
    "            if !haskey(global_metrics, genre)\n",
    "                # Initialize metrics if genre not in global metrics\n",
    "                global_metrics[genre] = Dict(\"count\" => 0.0, \"total_rating\" => 0.0, \"avg_rating\" => 0.0)\n",
    "            end\n",
    "\n",
    "            # Update global metrics for the genre\n",
    "            global_metrics[genre][\"count\"] += metrics[\"count\"]\n",
    "            global_metrics[genre][\"total_rating\"] += metrics[\"total_rating\"]\n",
    "            global_metrics[genre][\"avg_rating\"] = global_metrics[genre][\"total_rating\"] / global_metrics[genre][\"count\"]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return global_metrics, worker_times\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Worker (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function Worker(ratings_chunk::DataFrame, movies::DataFrame)\n",
    "    start_time = time_ns()  # Start time in nanoseconds\n",
    "\n",
    "    # Join ratings and movies on `movieId`\n",
    "    merged_data = innerjoin(ratings_chunk, movies, on = :movieId)\n",
    "\n",
    "    # Initialize an empty dictionary to store metrics per genre\n",
    "    genre_metrics = Dict{String, Dict{String, Float64}}()\n",
    "\n",
    "    # Process each row in the merged data\n",
    "    for row in eachrow(merged_data)\n",
    "        rating = row[:rating]\n",
    "        genres = lowercase.(strip.(split(row[:genres], '|'))) # Split genres and normalize\n",
    "\n",
    "        for genre in genres\n",
    "            # Initialize genre metrics if not already present\n",
    "            if !haskey(genre_metrics, genre)\n",
    "                genre_metrics[genre] = Dict(\"count\" => 0.0, \"total_rating\" => 0.0)\n",
    "            end\n",
    "            \n",
    "            # Update metrics for this genre\n",
    "            genre_metrics[genre][\"count\"] += 1\n",
    "            genre_metrics[genre][\"total_rating\"] += rating\n",
    "        end\n",
    "    end\n",
    "\n",
    "    elapsed_time = (time_ns() - start_time) / 1e9  # Time in seconds\n",
    "    return (JSON.json(genre_metrics), elapsed_time) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "save_metrics_to_csv (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function save_metrics_to_csv(global_metrics::Dict{String, Dict{String, Float64}}, file_path::String)\n",
    "    # Convert global_metrics to a DataFrame\n",
    "    metrics_data = DataFrame(\n",
    "        genre = String[], \n",
    "        count = Float64[], \n",
    "        ratings_sum = Float64[], \n",
    "        ratings_avg = Float64[]\n",
    "    )\n",
    "    \n",
    "    for (genre, metrics) in global_metrics\n",
    "        # Append each genre's metrics as a new row\n",
    "        push!(metrics_data, (\n",
    "            genre, \n",
    "            metrics[\"count\"], \n",
    "            metrics[\"total_rating\"], \n",
    "            metrics[\"avg_rating\"]\n",
    "        ))\n",
    "    end\n",
    "\n",
    "    # Write the DataFrame to CSV\n",
    "    CSV.write(file_path, metrics_data)\n",
    "end\n",
    "\n",
    "# save_metrics_to_csv(global_metrics, \"global_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Launching worker 1.0\n",
      "Launching worker 2.0\n",
      "Launching worker 3.0\n",
      "Launching worker 4.0\n",
      "Launching worker 5.0\n",
      "Launching worker 6.0\n",
      "Launching worker 7.0\n",
      "Launching worker 8.0\n",
      "Launching worker 9.0\n",
      "Launching worker 10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "420.480890181"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dyn_file_path = \"ratings.csv\"\n",
    "stat_file_path = \"movies.csv\"\n",
    "\n",
    "movies=CSV.read(stat_file_path,DataFrame)\n",
    "\n",
    "# Total rows: 25_000_095\n",
    "chunk_size=2500010\n",
    "map_reduce_time = @elapsed global_metrics, worker_times = Master(file_path=dyn_file_path, chunk_size=chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Reduce Time: 420.48 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"global_metrics_staggered.csv\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_metrics_to_csv(global_metrics, \"global_metrics_staggered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg worker time: 9.8461586344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10-element Vector{Any}:\n",
       "  9.966648177\n",
       "  9.857586486\n",
       "  9.836530276\n",
       "  9.832534818\n",
       "  9.860832938\n",
       "  9.582332391\n",
       "  9.81158842\n",
       "  9.718204708\n",
       " 10.113913467\n",
       "  9.881414663"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(\"Avg worker time: $(mean(worker_times))\")\n",
    "worker_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large DataSet Full Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Master (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function Master(; chunks::Vector{DataFrame})\n",
    "    \n",
    "    # Launch a worker for the current chunk\n",
    "    threads = [Threads.@spawn Worker(DataFrame(chunk),movies) for chunk in chunks]\n",
    "\n",
    "    # Collect the results from each worker\n",
    "    local_metrics_with_time = [fetch(thread) for thread in threads]\n",
    "\n",
    "    # Aggregate the local metrics from each worker\n",
    "    \n",
    "    # Initialize global metrics dictionary\n",
    "    global_metrics = Dict{String, Dict{String, Float64}}()\n",
    "    worker_times = []\n",
    "\n",
    "    # Aggregate metrics across all chunks\n",
    "    for (local_metric, elapsed_time) in local_metrics_with_time\n",
    "        local_data = JSON.parse(local_metric)  # Parse JSON string to dictionary\n",
    "        push!(worker_times, elapsed_time)\n",
    "\n",
    "        for (genre, metrics) in local_data\n",
    "            if !haskey(global_metrics, genre)\n",
    "                # Initialize metrics if genre not in global metrics\n",
    "                global_metrics[genre] = Dict(\"count\" => 0.0, \"total_rating\" => 0.0, \"avg_rating\" => 0.0)\n",
    "            end\n",
    "\n",
    "            # Update global metrics for the genre\n",
    "            global_metrics[genre][\"count\"] += metrics[\"count\"]\n",
    "            global_metrics[genre][\"total_rating\"] += metrics[\"total_rating\"]\n",
    "            global_metrics[genre][\"avg_rating\"] = global_metrics[genre][\"total_rating\"] / global_metrics[genre][\"count\"]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return global_metrics, worker_times\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split_dataframe (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function split_dataframe(df::DataFrame, n::Int)\n",
    "    chunk_size = ceil(Int, nrow(df) / n)\n",
    "    return [df[(i-1)*chunk_size+1:min(i*chunk_size, nrow(df)), :] for i in 1:n]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.986152028"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "movies=CSV.read(stat_file_path,DataFrame)\n",
    "@elapsed ratings=CSV.read(dyn_file_path,DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full DataSet Load Time: 4.98 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103.349820718"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_time = @elapsed rating_chunks = split_dataframe(ratings, 10)\n",
    "map_reduce_time = @elapsed global_metrics, worker_times = Master(chunks=rating_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Reduce Time: 103.35 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"global_metrics_full_load.csv\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_metrics_to_csv(global_metrics, \"global_metrics_full_load.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg worker time: 10.307402001099998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10-element Vector{Any}:\n",
       " 10.482768456\n",
       " 10.585821978\n",
       " 10.279204803\n",
       " 10.372483014\n",
       " 10.030660162\n",
       " 10.124822824\n",
       " 10.555043213\n",
       " 10.429826119\n",
       " 10.075788191\n",
       " 10.137601251"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(\"Avg worker time: $(mean(worker_times))\")\n",
    "worker_times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.0",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
