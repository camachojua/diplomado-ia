{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9Net6SsKNrm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvzeMZbLPI8f"
      },
      "source": [
        "## 1.2.- Implementar AlexNet en tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDY05qxtPPrz",
        "outputId": "24339247-cb7c-4c58-9971-26370a07ef79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 94ms/step - accuracy: 0.3504 - loss: 1.7495 - val_accuracy: 0.6438 - val_loss: 1.0133\n",
            "Epoch 2/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 91ms/step - accuracy: 0.6596 - loss: 0.9717 - val_accuracy: 0.7348 - val_loss: 0.7634\n",
            "Epoch 3/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 93ms/step - accuracy: 0.7554 - loss: 0.7043 - val_accuracy: 0.7495 - val_loss: 0.7764\n",
            "Epoch 4/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 93ms/step - accuracy: 0.8130 - loss: 0.5417 - val_accuracy: 0.7784 - val_loss: 0.6636\n",
            "Epoch 5/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 86ms/step - accuracy: 0.8601 - loss: 0.4066 - val_accuracy: 0.7821 - val_loss: 0.7090\n",
            "Epoch 6/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 93ms/step - accuracy: 0.8965 - loss: 0.3039 - val_accuracy: 0.7801 - val_loss: 0.7702\n",
            "Epoch 7/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 87ms/step - accuracy: 0.9208 - loss: 0.2305 - val_accuracy: 0.7762 - val_loss: 0.8211\n",
            "Epoch 8/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 86ms/step - accuracy: 0.9421 - loss: 0.1686 - val_accuracy: 0.7808 - val_loss: 0.9105\n",
            "Epoch 9/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 87ms/step - accuracy: 0.9512 - loss: 0.1384 - val_accuracy: 0.8004 - val_loss: 0.8609\n",
            "Epoch 10/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 85ms/step - accuracy: 0.9632 - loss: 0.1058 - val_accuracy: 0.8034 - val_loss: 0.8950\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f4318d6f410>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Cargar CIFAR-10\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalizar imágenes\n",
        "x_train, x_test = (x_train / 255.0) * 2 - 1, (x_test / 255.0) * 2 - 1\n",
        "y_train, y_test = y_train.flatten(), y_test.flatten()\n",
        "\n",
        "# ✅ Crear función de preprocesamiento para redimensionar dinámicamente\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.resize(image, (227, 227))  # Se hace en cada batch, no todo a la vez\n",
        "    return image, label\n",
        "\n",
        "# ✅ Crear datasets con `tf.data` (evita OOM)\n",
        "batch_size = 64\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_ds = train_ds.map(preprocess).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_ds = test_ds.map(preprocess).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Definir AlexNet\n",
        "def build_alexnet():\n",
        "    model = Sequential([\n",
        "        Conv2D(96, kernel_size=11, strides=4, activation='relu', input_shape=(227, 227, 3)),\n",
        "        MaxPooling2D(pool_size=3, strides=2),\n",
        "\n",
        "        Conv2D(256, kernel_size=5, padding='same', activation='relu'),\n",
        "        MaxPooling2D(pool_size=3, strides=2),\n",
        "\n",
        "        Conv2D(384, kernel_size=3, padding='same', activation='relu'),\n",
        "        Conv2D(384, kernel_size=3, padding='same', activation='relu'),\n",
        "        Conv2D(256, kernel_size=3, padding='same', activation='relu'),\n",
        "        MaxPooling2D(pool_size=3, strides=2),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# Crear y compilar el modelo\n",
        "model = build_alexnet()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# ✅ Entrenar modelo con batches pequeños\n",
        "model.fit(train_ds, epochs=10, validation_data=test_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3QvPkOu5jvk"
      },
      "source": [
        "\n",
        "#2.- Ahora vamos a comparar con los modelos preentrenados\n",
        "\n",
        "1.   Pytorch\n",
        "2.   Tensor-flow\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVdTkR5bA-TG"
      },
      "source": [
        "## 2.2.- Usar modelos preentrenados en TensorFlow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBzNF8j9UmRD",
        "outputId": "f3a14b3c-99ec-452c-e854-9537409f0031"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
            "Expected: ['keras_tensor_187']\n",
            "Received: inputs=Tensor(shape=(None, 224, 224, 3))\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 46ms/step - accuracy: 0.3386 - loss: 1.8026 - val_accuracy: 0.4393 - val_loss: 1.6150 - learning_rate: 5.0000e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 38ms/step - accuracy: 0.5310 - loss: 1.3145 - val_accuracy: 0.5246 - val_loss: 1.3532 - learning_rate: 5.0000e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 39ms/step - accuracy: 0.5725 - loss: 1.2043 - val_accuracy: 0.3927 - val_loss: 1.9133 - learning_rate: 5.0000e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 38ms/step - accuracy: 0.6013 - loss: 1.1357 - val_accuracy: 0.5654 - val_loss: 1.2304 - learning_rate: 5.0000e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 39ms/step - accuracy: 0.6216 - loss: 1.0712 - val_accuracy: 0.5830 - val_loss: 1.1892 - learning_rate: 5.0000e-05\n",
            "Epoch 6/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 39ms/step - accuracy: 0.6390 - loss: 1.0219 - val_accuracy: 0.5377 - val_loss: 1.3490 - learning_rate: 5.0000e-05\n",
            "Epoch 7/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 38ms/step - accuracy: 0.6557 - loss: 0.9793 - val_accuracy: 0.5679 - val_loss: 1.2597 - learning_rate: 5.0000e-05\n",
            "Epoch 8/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 37ms/step - accuracy: 0.6634 - loss: 0.9475 - val_accuracy: 0.4875 - val_loss: 1.5575 - learning_rate: 5.0000e-05\n",
            "Epoch 9/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 38ms/step - accuracy: 0.7146 - loss: 0.8173 - val_accuracy: 0.6763 - val_loss: 0.9428 - learning_rate: 1.0000e-05\n",
            "Epoch 10/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 39ms/step - accuracy: 0.7359 - loss: 0.7534 - val_accuracy: 0.6658 - val_loss: 0.9747 - learning_rate: 1.0000e-05\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 27ms/step - accuracy: 0.6685 - loss: 0.9670\n",
            "Test accuracy: 0.6658\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Habilitar Mixed Precision para reducir uso de RAM\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Cargar CIFAR-10\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalizar pixel values a [-1, 1]\n",
        "x_train, x_test = (x_train / 255.0) * 2 - 1, (x_test / 255.0) * 2 - 1\n",
        "\n",
        "# Aplanar etiquetas\n",
        "y_train, y_test = y_train.flatten(), y_test.flatten()\n",
        "\n",
        "# Función de preprocesamiento para Dataset\n",
        "def preprocess(img, label):\n",
        "    img = tf.image.resize(img, (224, 224))  # Redimensiona sobre la marcha\n",
        "    return img, label\n",
        "\n",
        "# Crear datasets de entrenamiento y prueba con tf.data\n",
        "batch_size = 32\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_ds = train_ds.shuffle(len(x_train)).map(preprocess).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_ds = test_ds.map(preprocess).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Definir input\n",
        "input_tensor = Input(shape=(224, 224, 3))\n",
        "\n",
        "# Cargar ResNet50 pre-entrenado\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
        "\n",
        "# Descongelar solo las últimas 15 capas para reducir uso de memoria\n",
        "for layer in base_model.layers[:-15]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[-15:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Añadir capas personalizadas\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu', dtype='float32')(x)  # Forzar capa en float32\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu', dtype='float32')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "output_layer = Dense(10, activation='softmax', dtype='float32')(x)\n",
        "\n",
        "# Crear modelo\n",
        "model = Model(inputs=base_model.input, outputs=output_layer)\n",
        "#model = Model(inputs=[base_model.input], outputs=output_layer)\n",
        "\n",
        "# Compilar modelo con un learning rate más bajo\n",
        "model.compile(optimizer=Adam(learning_rate=0.00005),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Callback para reducir learning rate si la precisión se estanca\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy',\n",
        "                              factor=0.2,\n",
        "                              patience=3,\n",
        "                              min_lr=0.00001)\n",
        "\n",
        "# Entrenar modelo\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=10,\n",
        "    validation_data=test_ds,\n",
        "    callbacks=[reduce_lr]\n",
        ")\n",
        "\n",
        "# Evaluar modelo\n",
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}