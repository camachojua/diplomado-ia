{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lhj1hNrC5sB0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Carga de librerías con optimizaciones de memoria\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import gc\n",
        "\n",
        "# Configuración para evitar problemas de memoria\n",
        "def configure_memory():\n",
        "    # Liberar memoria no utilizada\n",
        "    gc.collect()\n",
        "\n",
        "    # Configurar crecimiento de memoria de GPU\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            print(\"GPU memory growth enabled\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Memory growth configuration error: {e}\")\n",
        "\n",
        "    # Usar mixed precision para ahorrar memoria\n",
        "    try:\n",
        "        from tensorflow.keras import mixed_precision\n",
        "        policy = mixed_precision.Policy('mixed_float16')\n",
        "        mixed_precision.set_global_policy(policy)\n",
        "        print(\"Mixed precision enabled (float16)\")\n",
        "    except:\n",
        "        print(\"Could not enable mixed precision\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RwCQeekC7XuU"
      },
      "outputs": [],
      "source": [
        "# 2. Definición de carga de datos optimizada\n",
        "\n",
        "def get_data_loaders(batch_size=64, pretrained=False):\n",
        "    # Cargar CIFAR-10\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    # Reducir tamaño de imagen para modelos preentrenados\n",
        "    # (224x224 es demasiado grande para Colab, usamos 128x128)\n",
        "    target_size = 128 if pretrained else 32\n",
        "\n",
        "    # Preprocesamiento usando tf.data para eficiencia de memoria\n",
        "    def preprocess(x, y):\n",
        "        # Convertir a float32\n",
        "        x = tf.cast(x, tf.float32) / 255.0\n",
        "\n",
        "        # Redimensionar si es necesario\n",
        "        if pretrained and x.shape[0] != target_size:\n",
        "            x = tf.image.resize(x, (target_size, target_size))\n",
        "\n",
        "        # Normalización\n",
        "        if pretrained:\n",
        "            mean = tf.constant([0.485, 0.456, 0.406])\n",
        "            std = tf.constant([0.229, 0.224, 0.225])\n",
        "        else:\n",
        "            mean = tf.constant([0.5, 0.5, 0.5])\n",
        "            std = tf.constant([0.5, 0.5, 0.5])\n",
        "\n",
        "        x = (x - mean) / std\n",
        "        return x, y\n",
        "\n",
        "    # Crear datasets con procesamiento eficiente\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    train_ds = train_ds.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "    test_ds = test_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_ds, test_ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pssz-RDF7ebL"
      },
      "outputs": [],
      "source": [
        "# 3. Modelo AlexNet optimizado para memoria\n",
        "\n",
        "def AlexNetOptimized(num_classes=10):\n",
        "    model = models.Sequential([\n",
        "        # Capas de características (reducidas para ahorrar memoria)\n",
        "        layers.Conv2D(32, kernel_size=3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
        "        layers.MaxPooling2D(pool_size=2),\n",
        "        layers.Conv2D(64, kernel_size=3, padding='same', activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=2),\n",
        "        layers.Conv2D(128, kernel_size=3, padding='same', activation='relu'),\n",
        "        layers.Conv2D(128, kernel_size=3, padding='same', activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=2),\n",
        "\n",
        "        # Capas clasificadoras (reducidas para ahorrar memoria)\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EhzAotlh7nFi"
      },
      "outputs": [],
      "source": [
        "# 4. Funciones de entrenamiento y evaluación con checkpoints\n",
        "\n",
        "def train_model(model, train_ds, criterion, optimizer, num_epochs=10):\n",
        "    # Callbacks para mejor gestión de memoria\n",
        "    callbacks = [\n",
        "    ]\n",
        "\n",
        "    # Compilar modelo\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=criterion,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Entrenamiento\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        # Ajustar tasa de aprendizaje\n",
        "        if epoch > 0 and epoch % 5 == 0:\n",
        "            optimizer.learning_rate = optimizer.learning_rate * 0.5\n",
        "            print(f\"Learning rate adjusted to: {optimizer.learning_rate.numpy()}\")\n",
        "\n",
        "        # Entrenar por una época\n",
        "        history = model.fit(\n",
        "            train_ds,\n",
        "            epochs=1,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Liberar memoria después de cada época\n",
        "        gc.collect()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {history.history['loss'][0]:.4f}\")\n",
        "\n",
        "def evaluate_model(model, test_ds):\n",
        "    loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
        "    print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XqxH_r7u7rrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df46a12a-6916-4126-a379-da9453385384"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU memory growth enabled\n",
            "Mixed precision enabled (float16)\n",
            "\n",
            "===== Entrenando AlexNet optimizado =====\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.3048 - loss: 1.8582\n",
            "Epoch 1/10, Loss: 1.6185\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.5435 - loss: 1.2643\n",
            "Epoch 2/10, Loss: 1.2035\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.6232 - loss: 1.0619\n",
            "Epoch 3/10, Loss: 1.0297\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.6697 - loss: 0.9434\n",
            "Epoch 4/10, Loss: 0.9192\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6963 - loss: 0.8641\n",
            "Epoch 5/10, Loss: 0.8525\n",
            "Learning rate adjusted to: 0.004999999888241291\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7512 - loss: 0.7231\n",
            "Epoch 6/10, Loss: 0.6893\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7694 - loss: 0.6542\n",
            "Epoch 7/10, Loss: 0.6374\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7872 - loss: 0.6070\n",
            "Epoch 8/10, Loss: 0.5932\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7990 - loss: 0.5767\n",
            "Epoch 9/10, Loss: 0.5636\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8073 - loss: 0.5420\n",
            "Epoch 10/10, Loss: 0.5329\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7841 - loss: 0.6342\n",
            "Model Accuracy: 78.61%\n"
          ]
        }
      ],
      "source": [
        "# 5. Entrenamiento y evaluación con control de memoria\n",
        "\n",
        "def main():\n",
        "    # Configurar memoria\n",
        "    configure_memory()\n",
        "\n",
        "    print(\"\\n===== Entrenando AlexNet optimizado =====\")\n",
        "\n",
        "    # Obtener datos con batch pequeño\n",
        "    train_ds, test_ds = get_data_loaders(batch_size=32)\n",
        "\n",
        "    # Crear modelo optimizado\n",
        "    model = AlexNetOptimized()\n",
        "\n",
        "    # Parámetros de entrenamiento\n",
        "    criterion = 'sparse_categorical_crossentropy'\n",
        "    optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "    # Entrenar y evaluar\n",
        "    train_model(model, train_ds, criterion, optimizer, num_epochs=10)\n",
        "    evaluate_model(model, test_ds)\n",
        "\n",
        "    # Liberar memoria antes de salir\n",
        "    del model\n",
        "    gc.collect()\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}