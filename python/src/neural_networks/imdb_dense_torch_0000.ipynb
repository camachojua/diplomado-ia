{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  IMDB_Dense_Torch_0000\n",
    "\n",
    "1. Run under a PyTorch virtual env\n",
    "\n",
    "2. Take a deep dive to examine the data structures that the PyTorch Dense models need to process data similar to imdb data.\n",
    "\n",
    "3. Produce fake data that mimics the imdb data; process that data with PyTorch dense models. \n",
    "\n",
    "4. Since we know the nature of the artificial data, the processing results should be consistent with that data  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create artificial data\n",
    "# \n",
    "# Mimic the IMDB Data, which is typically set as list of (review, label) tuples.\n",
    "# The reviews are lists of word indices, from 1...10000, going from more important to less important\n",
    "# The labels are 0 or 1, indicating a bad or good review, respectively.\n",
    "\n",
    "# I created the data manually, to ensure that the data preprocessing, the models etc, work well.\n",
    "# There are only three different lists in this data to ensure that models can learn and predict\n",
    "# with high accuracy. Results with low accuracy will reveal problems in the data pre-processing,\n",
    "# the models configurations, or both...\n",
    "\n",
    "imdb_fake_data = [\n",
    "    ([1, 2, 3, 4, 5], 1),  # Review: [word1, word2, ...], Label: 1\n",
    "    ([6, 7, 8], 0),        # Review: [word1, word2, ...], Label: 0\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([6, 7, 8], 0),\n",
    "    ([6, 7, 8], 0),\n",
    "    ([6, 7, 8], 0),\n",
    "    ([6, 7, 8], 0),\n",
    "    ([6, 7, 8], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    # ... more data\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### IMDBDataset class\n",
    "\n",
    "Define class IMDBDataset for PyTorch's DataLoader.\n",
    "\n",
    "The class helps to process the movie reviews and the sentiment labels associated to each review [0,1].\n",
    "\n",
    "1. The class inherits from PyTorch's Dataset class\n",
    "\n",
    "2. ```__init__(self, data) ```  is the class constructor, which is called when a class instance is created.\n",
    "\n",
    "3. ```self``` is a reference to the instance calling the function. ```self ``` is used to access and modify instance variables.\n",
    "\n",
    "4. ```data``` is the argument expected when the class is executed. This is a Python obj (list of tuples) with a review and its label.\n",
    "\n",
    "5. ```self.data = data``` is the line that stores the arg data into the self.data. This makes the data accessible throughout the class's methods.\n",
    "\n",
    "6. ```def __len__(self)``` is a method that is **required** by PyTorch Dataset class. It returns the number of items in the dataset.\n",
    "\n",
    "7. ```def __getitem__(self, idx) ``` is a crucial method **required** by PyTorch DataSet. The method fetches and returns an obj that contains a review and a label, given its index.\n",
    "\n",
    "8. ```idx``` is the index of the data item to retrieve.\n",
    "\n",
    "9. ```return torch.tensor(review), torch.tensor(label)``` is a critical step for PyTorch. It converts the review and the label into PyTorch tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# 2. IMDBDataset(Dataset) is a class derived from torch.Dataset.\n",
    "# Therefore understanding how to especialize this class from the base class is crucial.\n",
    "# \n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review, label = self.data[idx]\n",
    "        return torch.tensor(review), torch.tensor(label)  # Convert to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### collate_fn\n",
    "\n",
    "+ The function  is designed for helping PyTorch's DataLoader handle batches of variable-length reviews.  \n",
    "\n",
    "+ The function addresses the need for padding sequences to a uniform length when working with neural networks.\n",
    "\n",
    "\n",
    "1. ``` def collate_fn(batch)``` defines the function and gets a batch as input.\n",
    "\n",
    "2. A batch is a list of data points. Each data point is what the method in IMDBDataset ``` __getitem__ ``` method returns. In this case, each data point is a tuple of (review, label).\n",
    "\n",
    "3. The line ```reviews, labels = zip(*batch)``` unpacks the batch. ```zip(*batch)``` also transposes the batch. If the batch is of the form ``` [(review1, label1), (review2, label2), ...]``` then ``` zip(*batch)``` will transform it into ```([review1, review2, ...], [label1, label2, ...])```. Therefore reviews becomes a tuple containing all the reviews in the batch, and labels becomes a tuple containing all the labels.\n",
    "\n",
    "4. The line ```review_lengths = torch.tensor([len(r) for r in reviews])``` is a crucial step, because it calculates the length of each review in the batch and stores these lengths in a PyTorch tensor called ```review_lengths```.  This is important because after padding, all reviews will have the same length, but the original lengths might be needed for other NLP tasks (e.g., when using recurrent neural networks, we don't want to process the padding).\n",
    "\n",
    "\n",
    "5. The line ```padded_reviews = pad_sequence(reviews, batch_first=True, padding_value=0)``` performs the padding.  ```pad_sequence``` is a PyTorch utility that takes a list of sequences (reviews in this case) and pads them to the length of the longest sequence in the batch.\n",
    "\n",
    "6. ```reviews``` is the list of reviews to pad.\n",
    "\n",
    "7. ```batch_first=True``` tells pad_sequence to return the padded sequences in the shape ```(batch_size, max_sequence_length)```. If this arg is False (the default), it would return instead ```(max_sequence_length, batch_size)```.  ```batch_first=True``` is more common.\n",
    "\n",
    "\n",
    "8. ``` padding_value=0``` specifies the value used for padding. \n",
    "\n",
    "9. The line ```labels = torch.stack(labels) ``` converts the tuple of labels into a PyTorch tensor. \n",
    "\n",
    "10. ```torch.stack``` concatenates the labels along a new dimension.  Since the labels are single values (0,1), this creates a tensor of shape (batch_size,).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Loading with Padding\n",
    "def collate_fn(batch):\n",
    "    reviews, labels = zip(*batch)\n",
    "    review_lengths = torch.tensor([len(r) for r in reviews])  # Store original lengths\n",
    "    padded_reviews = pad_sequence(reviews, batch_first=True, padding_value=0) # Pad to max length in batch\n",
    "    labels = torch.stack(labels) # Stack labels\n",
    "    return padded_reviews, labels, review_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test data. Training gets 80% of the data\n",
    "train_data, test_data = train_test_split(imdb_fake_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = IMDBDataset(train_data)\n",
    "test_dataset = IMDBDataset(test_data)\n",
    "\n",
    "## train_loader will be used during the training loops to load train data into the model\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "## test_loader will be used during the evalation loops to load test data into the model\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " #4. Define a Pytorch Dense Model\n",
    "\n",
    "class DenseModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super(DenseModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)  # Word embeddings\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # Create mask *before* embedding. 1 for non-padding, 0 for padding\n",
    "        mask = x != 0  # (batch_size, seq_len) - True for non-padding, False for padding\n",
    "        masked_embedded = embedded * mask.unsqueeze(-1).float() # Apply mask to embeddings\n",
    "\n",
    "        # Average pooling over sequence length (handles variable lengths)\n",
    "        lengths = lengths.unsqueeze(1).float()  # (batch_size, 1)\n",
    "        pooled = masked_embedded.sum(dim=1) / lengths  # Average pool\n",
    "\n",
    "        x = self.fc1(pooled)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Training and Validation Loops\n",
    "# input_dim = 10000  # Adjust size of vocabulary to 10000 \n",
    "input_dim = 100\n",
    "embedding_dim = 128  # Size of word embeddings\n",
    "hidden_dim = 256\n",
    "output_dim = 2  # Binary classification (0 or 1)\n",
    "\n",
    "model = DenseModel(input_dim, embedding_dim, hidden_dim, output_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # use gpu if available\n",
    "model.to(device) # move model to gpu\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7322, Accuracy: 100.00%\n",
      "Epoch [2/10], Loss: 0.5877, Accuracy: 100.00%\n",
      "Epoch [3/10], Loss: 0.4747, Accuracy: 100.00%\n",
      "Epoch [4/10], Loss: 0.3853, Accuracy: 100.00%\n",
      "Epoch [5/10], Loss: 0.3146, Accuracy: 100.00%\n",
      "Epoch [6/10], Loss: 0.2578, Accuracy: 100.00%\n",
      "Epoch [7/10], Loss: 0.2118, Accuracy: 100.00%\n",
      "Epoch [8/10], Loss: 0.1740, Accuracy: 100.00%\n",
      "Epoch [9/10], Loss: 0.1428, Accuracy: 100.00%\n",
      "Epoch [10/10], Loss: 0.1171, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# change number of epochs as necessary with the real IMDB data.\n",
    "#  Maybe just 10 will be enough \n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()  # Set model to training mode\n",
    "    for padded_reviews, labels, lengths in train_loader:\n",
    "      padded_reviews = padded_reviews.to(device) # move data to gpu\n",
    "      labels = labels.to(device) # move data to gpu\n",
    "      lengths = lengths.to(device) # move data to gpu\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(padded_reviews, lengths)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradients during validation\n",
    "        for padded_reviews, labels, lengths in test_loader:\n",
    "          padded_reviews = padded_reviews.to(device) # move data to gpu\n",
    "          labels = labels.to(device) # move data to gpu\n",
    "          lengths = lengths.to(device) # move data to gpu\n",
    "          outputs = model(padded_reviews, lengths)\n",
    "          _, predicted = torch.max(outputs.data, 1) # get the prediction\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item() # count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 100.00%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4 0]\n",
      " [0 1]]\n",
      "Prediction for example review: 1\n"
     ]
    }
   ],
   "source": [
    "# 6. Evaluation (After Training) - More Detailed\n",
    "model.eval()  # Set to evaluation mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # use gpu if available\n",
    "model.to(device) # move model to gpu\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for padded_reviews, labels, lengths in test_loader:\n",
    "        padded_reviews = padded_reviews.to(device) # move data to gpu\n",
    "        labels = labels.to(device) # move data to gpu\n",
    "        lengths = lengths.to(device) # move data to gpu\n",
    "        outputs = model(padded_reviews, lengths)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())  # Store predictions for later analysis\n",
    "        all_labels.extend(labels.cpu().numpy())      # Store true labels\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# 7. Additional Evaluation Metrics (using scikit-learn)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_predictions))\n",
    "\n",
    "\n",
    "# Example of getting predictions for a single review\n",
    "def predict(review_indices):\n",
    "    model.eval()\n",
    "    review_tensor = torch.tensor([review_indices]).to(device)  # Add batch dimension\n",
    "    review_length = torch.tensor([len(review_indices)]).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(review_tensor, review_length)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    return predicted.item()\n",
    "\n",
    "# Example usage:\n",
    "example_review = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Replace with actual review indices\n",
    "prediction = predict(example_review)\n",
    "print(f\"Prediction for example review: {prediction}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
