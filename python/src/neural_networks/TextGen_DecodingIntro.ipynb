{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Decoding Intro  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction**\n",
    "\n",
    "In recent years, there has been an increased interest in language generation due to the rise of transformer-based language models trained on millions of webpages, such as OpenAI's famous [GPT2 model](https://openai.com/blog/better-language-models/).\n",
    "\n",
    "The results on conditioned open-ended language generation are quite good, e.g. [GPT2 on unicorns](https://openai.com/blog/better-language-models/#samples), [XLNet](https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e), [XLNet @ GitHub](https://github.com/zihangdai/xlnet/)\n",
    "\n",
    "\n",
    "Besides the improved transformer architecture and massive unsupervised training data, **better decoding methods** have also played an important role.\n",
    "\n",
    "This notebook is an intro to some of the decoding methods used for text generation. The notebook gives a brief overview of different decoding strategies and shows how to implement them with very little effort using the popular [Huggin Face Transformers Library](https://github.com/huggingface/transformers)\n",
    "\n",
    "The notebook briefly mentions the more general topic known as [auto-regressive language generation](http://jalammar.github.io/illustrated-gpt2/).\n",
    "\n",
    "In short, **auto-regressive** language generation is based on the assumption that the probability distribution of a word sequence can be decomposed into the product of conditional next word distributions as in the formula below. Note that the formula is remarkably similar to the **autoregressive** [formulation](https://docs.google.com/presentation/d/1SB1fYiOAzDqZS_NSjb7zONKqdGS-vkXB5g_5eE8dt_0/edit#slide=id.g1ec32d6190d_0_137) we used for **RNNs** :\n",
    "\n",
    "$$ P(w_{1:T} | W_0 ) = \\prod_{t=1}^T P(w_{t} | w_{1: t-1}, W_0) \\text{ ,with }  w_{1: 0} = \\emptyset, $$\n",
    "\n",
    "where $W_0$ is the initial *context* word sequence. The length $T$ of the word sequence is usually determined *on-the-fly* and corresponds to the timestep $t=T$ the EOS token is generated from $P(w_{t} | w_{1: t-1}, W_{0})$.\n",
    "\n",
    "Auto-regressive language generation is available for `GPT2`, `XLNet`, `OpenAi-GPT`, `CTRL`, `TransfoXL`, `XLM`, `Bart`, `T5` in both PyTorch 2.5+ and TF.\n",
    "\n",
    "Here we visit the following decoding methods: \n",
    "\n",
    "1. *Greedy search*\n",
    "2. *Beam search*\n",
    "3. *Sampling*\n",
    "4. *Top-K sampling*\n",
    "5. *Top-p sampling*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Greedy Search**\n",
    "\n",
    "Greedy search selects the word with the highest probability as its next word: $w_t = argmax_{w}P(w | w_{1:t-1})$ at each timestep $t$. \n",
    "\n",
    "The figure below shows how greedy search navigates through sentence trellis\n",
    "\n",
    "![Greedy Search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/greedy_search.png)\n",
    "\n",
    "Starting from the word **\"The\"**, greedy search chooses the next word with the highest probability **\"nice\"** and so on, until it reaches the last  word. \n",
    "\n",
    "Once the search is complete, the sequence is **\"The\", \"nice\", \"woman\"** with an overall probability of $0.5 \\times 0.4 = 0.2$.\n",
    "\n",
    "In the next sections we will generate word sequences using **HF GPT2**  with the input text as context **( \"I\", \"enjoy\", \"walking\", \"with\", \"my\", \"cute\", \"dog\")**.\n",
    "\n",
    "We will also see how different encoding and serach methods can be used with **HF Transformers**\n",
    "\n",
    "\n",
    "[Huggin Face](https://github.com/huggingface)\n",
    "\n",
    "[Huggin Face Transformers](https://github.com/huggingface/transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import HTML\n",
    "\n",
    "# The code here is using Pytorch 2.6\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_after_char(text, char):\n",
    "  \"\"\"Removes all text from a string after the first occurrence of a given character.\n",
    "\n",
    "  Args:\n",
    "    text: The input string.\n",
    "    char: The character to search for.\n",
    "\n",
    "  Returns:\n",
    "    The string with all text after the character removed, or the original string if the character is not found.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    index = text.index(char)\n",
    "    return text[:index]  # Slice the string up to the character's index\n",
    "  except ValueError:\n",
    "    return text  # Return the original string if the character is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a text that contains two lines separated by \n",
      "\n",
      "  just like the lines that the decoders produce\n",
      "this is lufas: this is a text that contains two lines separated by \n"
     ]
    }
   ],
   "source": [
    "lofas = \"this is a text that contains two lines separated by \\n\\n  just like the lines that the decoders produce\"\n",
    "\n",
    "print( lofas ) \n",
    "lufas = remove_after_char(lofas, \"\\n\")\n",
    "print(\"this is lufas:\",  lufas ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drv3/hm3/code/python/torch2.6/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# The APIs here come from Huging Face. The HF people provide versions for TF and Torch. \n",
    "# The TF versions have names that start with TF. \n",
    "# See below \n",
    "\n",
    "# from transformers import TFGPT2LMHeadModel, GPT2Tokenizer # TF for TensorFlow\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer # For Torch \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEP CELL TO SHOW ERROR  \n",
    "\n",
    "#from transformers import AutoTokenizer, AutoModelForCausalLM #or other model class\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained('gpt2') # Or your model\n",
    "#model = AutoModelForCausalLM.from_pretrained('gpt2') # Or your model\n",
    "\n",
    "#input_ids_list = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='pt')\n",
    "#input_ids = torch.tensor([input_ids_list])  # Convert to PyTorch tensor\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "#greedy_output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "#print(\"Output:\\n\" + 100 * '-')\n",
    "#print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF AutoModelForCausalLM\n",
    "\n",
    "The HF Transformers is an API that has several pre-trained GPT-2 language models.\n",
    "\n",
    "** AutoModelForCausalLM** is a class from the HF Transformers library. \"Auto\" means that the library will automatically detect the correct model class based on the provided pre-trained checkpoint name (\"gpt2\" in this case).\n",
    "\n",
    "**ModelForCausalLM** specifies that we will use a model designed for causal language modeling. Causal language modeling is the task of predicting the next token in a sequence, given the previous tokens. This is the core functionality of models like GPT-2.\n",
    "\n",
    "**from_pretrained('gpt2')** is a method of the AutoModelForCausalLM class. It loads a pre-trained model and its configuration from a specified checkpoint. \n",
    "\n",
    "**'gpt2'** is the identifier for the GPT-2 model. It corresponds to a model that has been pretrained and is stored on the HF hub. The library will download the model's weights and configuration files.\n",
    "\n",
    "\n",
    "In essence\n",
    "\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "does the following:\n",
    "\n",
    "1. Identifies the model, i.e., it recognizes that we are using the GPT-2 model.\n",
    "\n",
    "2. Downloads the model, its pre-trained weights and configuration files for GPT-2 from the HF Model Hub (if they are not already cached locally).\n",
    "\n",
    "3. Instantiates the model, i.e., creates an instance of the appropriate model class (in this case, a causal language model) and loads the downloaded weights into it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# code in this cell is using greedy search under PyTorch (note the use of 'pt')\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt')\n",
    "\n",
    "# convert to list to fix error \n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, attention_mask=attention_mask, max_length=25)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations. We have generated a short text with GPT2 ðŸ˜Š.\n",
    "\n",
    "The generated words following the context are reasonable, but the model quickly starts repeating itself! This is a very common problem in language generation in general and seems to be even more so in greedy and beam search - check out [Vijayakumar et al., 2016](https://arxiv.org/abs/1610.02424) and [Shao et al., 2017](https://arxiv.org/abs/1701.03185).\n",
    "\n",
    "The major problem of greedy search is that it misses high probability words hidden behind a low probability word as can be seen in the figure above.\n",
    "\n",
    "If we go back to that figure, we can observe that the probability for the word **\"has\"** is a high $0.9$ but the word is hidden behind the word **\"dog\"**, which has only the second-highest conditional probability. Thus greedy search misses the word sequence **\"The\", \"dog\", \"has\"**.\n",
    "\n",
    "Beam Search is next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Beam search**\n",
    "\n",
    "Beam search reduces the risk of missing hidden word sequences of high probability by keeping the most likely `num_beams` of hypotheses at each step and eventually choosing the hypothesis that has the overall highest probability.\n",
    "\n",
    "Let's use  `num_beams=2`:\n",
    "\n",
    "![Beam search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n",
    "\n",
    "At step $1$, besides the most likely hypothesis **\"The\", \"nice\"**, beam search also keeps track of the second most likely one **\"The\", \"dog\"**.\n",
    "\n",
    "At step $2$, beam search finds that the word sequence **\"The\", \"dog\", \"has\"** with $0.4*0.9=0.36$ is a higher probability than **\"The\", \"nice\", \"woman\"**, which has $0.5*0.4=0.2$. \n",
    "\n",
    "Beam search found the most likely word sequence in this toy example, and in general, beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output.\n",
    "\n",
    "Let's see how beam search can be used in `transformers`. We set `num_beams > 1` and `early_stopping=True` so that generation is finished when all beam hypotheses reached the EOS token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# code in cell uses Beam Search under PyTorch.\n",
    "# Q: Where may we find more information about this code and what it does???? \n",
    "# A: At the link below:\n",
    "\n",
    "# https://huggingface.co/docs/transformers/main//generation_strategies\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# activate beam search and early_stopping.\n",
    "# How do we know that we are using beam search? Because the param num_beams, != 0 \n",
    "# Add attention_mask \n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask, # Add attention_mask here\n",
    "    max_length=60, # Note change from 50 -> 60 \n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "\n",
    "lofas = tokenizer.decode(beam_output[0])\n",
    "\n",
    "lufas = remove_after_char(lofas, \"\\n\")\n",
    "print( lufas ) \n",
    "\n",
    "#print(\"Output:\\n\" + 100 * '-')\n",
    "#print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the result is arguably more fluent, the output still includes repetitions of the same word sequences.  \n",
    "\n",
    "A simple remedy is to introduce *n-grams* (*a.k.a* word sequences of $n$ words) penalties as introduced by [Paulus et al. (2017)](https://arxiv.org/abs/1705.04304) and [Klein et al. (2017)](https://arxiv.org/abs/1701.02810). \n",
    "\n",
    "The most common *n-grams* penalty makes sure that no *n-gram* appears twice by manually setting the probability of next words that could create an already seen *n-gram* to $0$.\n",
    "\n",
    "Let's try it out by setting `no_repeat_ngram_size=2` so that no *2-gram* appears twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask, # Add attention_mask here\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "lofas = tokenizer.decode(beam_output[0]) \n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better. We can see that the repetition does not appear anymore. Nevertheless, *n-gram* penalties have to be used carefully. \n",
    "\n",
    "For example, articles generated about the city *New York* should not use a *2-gram* penalty, otherwise the name of the city would only appear once in the whole text!\n",
    "\n",
    "Another important feature about beam search is that we can compare the top beams after generation and choose the best beam gnerated for our purpose.\n",
    "\n",
    "This is done in HF `transformers`, by setting the parameter `num_return_sequences` in the call to model.generate( args ) \n",
    "to the number of highest scoring beams that the model should return.\n",
    "\n",
    "Make sure though that `num_return_sequences <= num_beams`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n",
      "\n",
      "\n",
      "1: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to get back to\n",
      "\n",
      "\n",
      "2: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n",
      "\n",
      "\n",
      "3: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to get back to\n",
      "\n",
      "\n",
      "4: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a step\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,  # <--- Add attention_mask here\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 5 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "    print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beam hypotheses shown above are only marginally different to each other. This should not be surprising because we are using small numbers of beams.\n",
    "\n",
    "When moving forward from simple examples to production open-ended generation, there are several reasons why beam search might not be the best option. For example:\n",
    "\n",
    "- Beam search can work very well in tasks where the length of the desired generation is more or less predictable as in machine translation or summarization.\n",
    "- See for example [Murray et al. (2018)](https://arxiv.org/abs/1808.10006) and [Yang et al. (2018)](https://arxiv.org/abs/1808.09582).\n",
    "- But this is not the case for open-ended generation where the desired output length can vary greatly, e.g. dialog and story generation.\n",
    "\n",
    "- We have seen that beam search suffers from repetitive word generation. \n",
    "\n",
    "- This is hard to control with *n-gram*- or other penalties in story generation.\n",
    "- Finding a good trade-off between forced \"no-repetition\" and repeating cycles of identical *n-grams* requires significant  finetuning.\n",
    "\n",
    "- As argued in the paper named \"The Curious Case of Neural Test Degeneration\" by [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751)\n",
    "\n",
    "![The Curious Case of Neural Text Degeneration]( CuriousCaseOfNeuralTextDeGeneration.png )\n",
    "\n",
    "\n",
    "high quality human language does not strictly follow a distribution of high probability next words.\n",
    "\n",
    "In other words, as humans, we often prefer text with some elements of surprise, so that the dialog is less boring and less predictable.\n",
    "\n",
    "The authors show in the figure as below, the probability that a model would give to human text vs. what beam search does.\n",
    "\n",
    "From that figure we can clearly conclude that the element of surprise is absent in beam search.\n",
    "\n",
    "![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)\n",
    "\n",
    "\n",
    "So let's stop being boring and introduce some randomness ðŸ¤ª."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# HTML('<img src=\"CuriousCaseOfNeuralTextDeGeneration.png\" alt=\" My PNG\", width=\"600\">')\n",
    "# Image(filename='/drv3/hm3/code/python/torch2.6/local/Transformer/CuriousCaseOfNeuralTextDeGeneration.png', width=300, height=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sampling**\n",
    "\n",
    "In most contexts, sampling means randomly picking the next word $w_t$ according to a conditional probability distribution that takes into consideration recent history:\n",
    "\n",
    "$$w_t \\sim P(w|w_{1:t-1})$$\n",
    "\n",
    "Taking the example from above, the following figure shows language generation when sampling.\n",
    "\n",
    "![vanilla_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/sampling_search.png)\n",
    "\n",
    "It looks like language generation using sampling is not *deterministic* anymore. \n",
    "\n",
    "The word **\"car\"** is sampled from the conditioned probability distribution P(w | \"The\"), followed by sampling \"drives\" from the distribution P(w | \"The\", \"car\").\n",
    "\n",
    "In `transformers`, we set `do_sample=True` and deactivate *Top-K* sampling (explained in the next cell) via `top_k=0`. \n",
    "\n",
    "In the following cell, we fix `random_seed=0` for illustration purposes. To get a better feel of what is happening, we could change the `random_seed` to play around with the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog when there are other people around, though.\n",
      "\n",
      "No, ladies, enjoying your dog and publicly embracing her is not my thing. It doesn't even bother me, woman-like. I'm happy you think\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "    torch.backends.cudnn.deterministic = True  #needed if using cuda\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "## Change this several times to show the effect \n",
    "set_seed(1) # Equivalent to tf.random.set_seed(0)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask, # Add attention mask here\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully there are less weird n-grams and the output is a bit more coherent now!\n",
    "\n",
    "While applying temperature can make a distribution less random, in its limit, when setting `temperature` $ \\to 0$, temperature scaled sampling becomes equal to greedy decoding and will suffer from the same problems as before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Top-K Sampling**\n",
    "\n",
    "[Fan et. al (2018)](https://arxiv.org/pdf/1805.04833.pdf) introduced a simple, but powerful sampling scheme, called ***Top-K*** sampling. \n",
    "\n",
    "In *Top-K* sampling, the *K* most likely next words are filtered and the probability mass is redistributed among only those *K* next words.\n",
    "\n",
    "GPT2 adopted this sampling scheme, which was one of the reasons for its success in story generation.\n",
    "\n",
    "In the example fro the cell above, we extended the range of words used for both sampling steps from 3 words to 10 words to better illustrate *Top-K* sampling.\n",
    "\n",
    "![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n",
    "\n",
    "Having set $K = 6$, in both sampling steps we limit our sampling pool to 6 words. \n",
    "\n",
    "Note tha in step 1 the 6 most likely words, defined as $V_{\\text{top-K}}$ encompass around two-thirds of the whole probability mass.\n",
    "\n",
    "In the second step the method includes almost all of the probability mass.\n",
    "\n",
    "Observe that the method successfully eliminates the other candidates *\"not\", \"the\", \"small\", \"told\"* in the second sampling step.\n",
    "\n",
    "Let's see how *Top-K* can be used in the library by setting `top_k=50`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog,\" she says. \"You get a lot of love and support out of it. It has helped me to be open and see why and what I have to do to be successful.\"\n",
      "\n",
      "I'd say the\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Change the seed though to get different results\n",
    "# tf.random.set_seed(0)\n",
    "\n",
    "set_seed(0) # Equivalent to tf.random.set_seed(0)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask, # Add attention mask here\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the text is arguably the most *human-sounding* text so far.\n",
    "\n",
    "One concern with *Top-K* sampling is that it does not dynamically adapt the number of words that are filtered from the next word probability distribution $P(w|w_{1:t-1})$.\n",
    "\n",
    "This can be problematic as some words might be sampled from a very sharp distribution (distribution on the right in the graph above), whereas others from a much more flat distribution (distribution on the left in the graph above).\n",
    "\n",
    "In step $t=1$, *Top-K* eliminates the possibility to sample *\"people\", \"big\", \"house\", \"cat\"*,  which seem like reasonable candidates. \n",
    "\n",
    "On the other hand, in step $t=2$ the method includes the arguably ill-fitted words *\"down\", \"a\"* in the sample pool of words.\n",
    "\n",
    "Thus, limiting the sample pool to a fixed size *K* could endanger the model to produce gibberish for sharp distributions and limit the model's creativity for flat distribution.\n",
    "\n",
    "This intuition led [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751) to create ***Top-p***- or ***nucleus***-sampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Top-p (nucleus) sampling**\n",
    "\n",
    "Instead of sampling only from the most likely *K* words, in *Top-p* sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability *p*. \n",
    "\n",
    "The probability mass is then redistributed among this set of words. In this way, the size of the set of words (*a.k.a* the number of words in the set) can dynamically increase or decrease according to the next word's probability distribution.\n",
    "\n",
    "The figure illustrates the concept\n",
    "\n",
    "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/top_p_sampling.png?raw=true)\n",
    "\n",
    "Having set $p=0.92$, *Top-p* sampling picks the *minimum* number of words to exceed a cumulative value of $p=92\\%$ of the probability mass, defined as $V_{\\text{top-p}}$. \n",
    "\n",
    "In the first example, this included the 9 most likely words, whereas it only has to pick the top 3 words in the second example to exceed 92%. It can be seen that this method keeps a wide range of words where the next word is arguably less predictable, *e.g.* P(w | \"The\"), and only a few words when the next word seems more predictable, *e.g.* P(w | \"The\", \"car\").\n",
    "\n",
    "We activate *Top-p* sampling by setting `0 < top_p < 1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog,\" she says. \"You get a lot of love and eventually a great guy comes in with your national credentials. He gives you a virtual identity as a dog owner. You get second chances. It's a fascinating\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Change the seed though to get different results\n",
    "# tf.random.set_seed(0)\n",
    "# Note the  params top_p = 0.92 and top_k=0\n",
    "# set them to dofferent values and compare results\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask, # Add attention mask here\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_p=0.92,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text now better. \n",
    "\n",
    "While in theory, *Top-p* seems more elegant than *Top-K*, both methods work well in practice.\n",
    "\n",
    "*Top-p* can also be used in combination with *Top-K*, which can avoid very low ranked words while allowing for some dynamic selection.\n",
    "\n",
    "Finally, to get multiple independently sampled outputs, we can *again* set the parameter `num_return_sequences > 1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog,\" she says. \"You get a lot of love and support out of it. It has helped me to be open and see what's really cool. I'm happy to see people are supporting my cause and just\n",
      "1: I enjoy walking with my cute dog. I would also like to see a new feature for our cats, the cute bear, that is called 'Spend Your Sunday, Beating Dogs, by Feeding Dogs'.\n",
      "\n",
      "Please see our page for\n",
      "2: I enjoy walking with my cute dog, but I would definitely encourage anyone that will play around with your dog's ears to use a bit of patience and patience.\n",
      "\n",
      "The dog's ears should be removed right away. After they are gone from the\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Change the seed though to get different results\n",
    "# tf.random.set_seed(0)\n",
    "# change params to compare results \n",
    "\n",
    "set_seed(0) # Equivalent to tf.random.set_seed(0)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have some tools we could use to write stories with `transformers`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusions, Part 1**\n",
    "\n",
    "*top-p* and *top-K* sampling are *ad-hoc* methods that seem to produce more fluent text than traditional *greedy* - and *beam* search on open-ended language generation.\n",
    "\n",
    "Recent evidence has revealed that the problems of *greedy* and *beam* search (mainly generating repetitive word sequences) are caused by the model (especially the way the model is trained), rather than the decoding methods, *cf.* [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf). \n",
    "\n",
    "Another author [Welleck et al. (2020)](https://arxiv.org/abs/2002.02492), mentions that *top-K* and *top-p* sampling also suffer from generating repetitive word sequences.\n",
    "\n",
    "In [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf), the authors show that according to human evaluations, *beam* search can generate more fluent text than *Top-p* sampling, when adapting the model's training objective.\n",
    "\n",
    "Open-ended language generation is a rapidly evolving field of research. As it is often the case, in rapid evolving fields of research there is no single solution to fit all.\n",
    "\n",
    "Therefore the best method is often the one that is discovered after experimenting with specific use cases. \n",
    "\n",
    "Experimentation is a good idea. Fortunately it is now possible (and easy) to try out several decoding methods using `transfomers` ðŸ¤—.\n",
    "\n",
    "That was a short introduction on how to use different decoding methods in `transformers` and recent trends in open-ended language generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusions, Part 2**\n",
    "\n",
    "There are a couple of additional parameters for the `generate` method that were not mentioned above. This last piece explain them here briefly!\n",
    "\n",
    "- `min_length` can be used to force the model to not produce an EOS token (= not finish the sentence) before `min_length` is reached. This is used quite frequently in summarization, but can be useful in general if the user wants to have longer outputs.\n",
    "\n",
    "- `repetition_penalty` can be used to penalize words that were already generated or belong to the context. It was first introduced by [Kesker et al. (2019)](https://arxiv.org/abs/1909.05858) and is also used in the training objective in [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf). It can be quite effective at preventing repetitions, but seems to be very sensitive to different models and use cases, *e.g.* see this [discussion](https://github.com/huggingface/transformers/pull/2303) on Github.\n",
    "\n",
    "- `attention_mask` can be used to mask padded tokens\n",
    "\n",
    "- `pad_token_id`, `bos_token_id`, `eos_token_id`: If the model does not have those tokens by default, the user can manually choose other token ids to represent them.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
