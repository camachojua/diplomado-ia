{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File Name = txtProc_BagOTrigrams.ipynb\n",
    "\n",
    "Do Txt Processing under Keras TF, specifically, do sentiment analysis using the\n",
    "bag of words approach\n",
    "\n",
    "Last tested           4/12/2024\n",
    "OS                    Ubuntu 22.04.4  LTS\n",
    "Python                3.10.6\n",
    "TF Version            2.16.1\n",
    "Keras version         2.16.1\n",
    "numpy version         1.26.4\n",
    "Number of GPUs        1\n",
    "nVidia Driver         550.54.15\n",
    "CUDA Version:          12.4  \n",
    "\n",
    "IMDB data ise used, from \n",
    "\n",
    "\n",
    "cd ~Data\n",
    "mkdir IMDB\n",
    "cd IMDB\n",
    "in path ~/Data/IMDB :\n",
    "curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "tar -xf aclImdb_v1.tar.gz\n",
    "\n",
    "The commands above create the following directory structure \n",
    "aclImdb/\n",
    "...train/\n",
    "......pos/\n",
    "......neg/\n",
    "...test/\n",
    "......pos/\n",
    "......neg/\n",
    "    \n",
    "The data was used as part of the paper \"Learning Word Vectors for Sentiment Analysis\" by Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and  Christopher Potts, all from Stanford, 2011.\n",
    "\n",
    "The \"readme.txt\" file in the main directory describes the organization of the data\n",
    "files, and the meainig of the names in the files.\n",
    "\n",
    "There are 25K text files for training and 25K for testing. The train/pos/ directory contains 12.5k text files, each of which contains positive-sentiment movie reviews Similarly, there are 12.5K negative-sentiment reviews located at the “neg” directory.\n",
    "\n",
    "The test/pos and test/neg directories follow the same idea. \n",
    "\n",
    "Before creating and running the model, a validation was created (once) by using  5K files from the training set, resulting in a change in size of the training as 25k - 5k = 20k.\n",
    "\n",
    "The unsup directory contains \"unsupervised\" data, which is not used.\n",
    "\n",
    "The code here uses different vectorization which is different from the code in bag_of_tokens. Here the TextVectorization specifies an additional argument ngrams=2, and it gives a different name to the ngrams produced.\n",
    "\n",
    "Once this is done, I think the code flows in the same way as in bag_of_tokens\n",
    "The model contains 16 (dense) layers. The activation function is \"relu\" and uses an input of size max_tokens=20000. \n",
    "\n",
    "Once these steps are done the model is created and when it runs, the validation accuracy is 90% so we went from 88% using a bag of tokens to 90% using 2grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 19:29:48.876531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741393788.889446   53104 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741393788.892970   53104 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-07 19:29:48.907989: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Version    2.18.0\n",
      "TF Path       /drv3/hm3/code/python/tf2.18/tf2.18/lib/python3.12/site-packages/keras/api/_v2\n",
      "Keras version  3.7.0\n",
      "numpy version  1.26.4\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from logging import logProcesses\n",
    "import os, pathlib, shutil, random\n",
    "from platform import python_branch\n",
    "from syslog import LOG_SYSLOG\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "\n",
    "print(\"TF Version   \", tf.__version__)\n",
    "print(\"TF Path      \", tf.__path__[0])\n",
    "print(\"Keras version \", keras.__version__)\n",
    "print(\"numpy version \", np.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "create_validation_set_fn()\n",
    "\n",
    "Take 20% of the training set for validation. The training set gives away 5k files,\n",
    "therefore after this function executes, the training set will contain 25k-5k=20k.\n",
    "Before running this function it is a good idea to remove the directory (if present).\n",
    "\n",
    "Last time I used this code was with  /drv3/hm3/Data/IMDB/aclImdb/val\n",
    "and its content. \n",
    "\n",
    "DO NOT RUN THIS CODE if the val data is already created\n",
    "\"\"\" \n",
    "def create_validation_set_fn() :\n",
    "  base_dir = pathlib.Path(\"/drv3/hm3/Data/IMDB/aclImdb\")\n",
    "  val_dir = base_dir / \"val\"\n",
    "  train_dir = base_dir / \"train\"\n",
    "\n",
    "  for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1377).shuffle(files)\n",
    "    num_val_samples = int(0.2*len(files))\n",
    "    val_files = files[ -num_val_samples: ]\n",
    "    for fname in val_files :\n",
    "      shutil.move( train_dir / category /fname, val_dir / category / fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create_data_sets_fn()\n",
    "\n",
    "trn_ds, tst_ds, val_ds = create_data_sets_fn( strBaseDir )\n",
    "  \n",
    "Rceive a string that contains a base directory where the raw data is stored.\n",
    "Return three data sets, for training, testing and validation\n",
    "  i.e., (trn, tst, val)\n",
    "\"\"\"\n",
    "def create_data_sets_fn( strBaseDir ) :\n",
    "    batch_size = 32\n",
    "    base_dir = pathlib.Path( strBaseDir )\n",
    "    train_ds =  keras.utils.text_dataset_from_directory( base_dir / \"train\", batch_size=batch_size )\n",
    "    val_ds = keras.utils.text_dataset_from_directory( base_dir / \"val\", batch_size=batch_size )\n",
    "    test_ds = keras.utils.text_dataset_from_directory( base_dir / \"test\", batch_size=batch_size )\n",
    "\n",
    "    # verify by printing that the 3 data sets were created correctly\n",
    "    for inputs, targets in train_ds:\n",
    "         print(\"inputs.shape:\", inputs.shape)\n",
    "         print(\"inputs.dtype:\", inputs.dtype)\n",
    "         print(\"targets.shape:\", targets.shape)\n",
    "         print(\"targets.dtype:\", targets.dtype)\n",
    "         print(\"inputs[0]:\", inputs[0])\n",
    "         print(\"targets[0]:\", targets[0])\n",
    "         break\n",
    "    \n",
    "    return train_ds, test_ds, val_ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "textvectorization_preprocessing_fn( train_ds, test_ds, val_ds)\n",
    "\n",
    "Get the three data sets as arguments to limit the vocabulary to the 20K\n",
    "most frequent words in the data set. \n",
    "\n",
    "Note that the argument to textVectorization sets the data as bigrams\n",
    "\n",
    " As part of ending, the function\n",
    "returns the items as shown below\n",
    "\n",
    "     return binary_2gram_train_ds, binary_2gram_train_ds, binary_2gram_train_ds\n",
    "\"\"\"\n",
    "def textvectorization_preprocessing_fn( train_ds, test_ds, val_ds ) :\n",
    "  text_vectorization = TextVectorization( ngrams=3, \n",
    "                                         max_tokens=20000, output_mode=\"multi_hot\",)\n",
    "    \n",
    "  # prepare a data set that yields only fields raw text input (no labels)\n",
    "  text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "  text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "  # prepare processed versions for training, validation, testing. Use 8 cores\n",
    "  binary_3gram_train_ds = train_ds.map ( lambda x, y: (text_vectorization(x), y), num_parallel_calls=8)\n",
    "  binary_3gram_test_ds  = test_ds.map ( lambda x, y: (text_vectorization(x), y), num_parallel_calls=8)\n",
    "  binary_3gram_val_ds   = val_ds.map ( lambda x, y: (text_vectorization(x), y), num_parallel_calls=8)\n",
    "\n",
    "  for inputs, targets in  binary_3gram_train_ds :\n",
    "    print(\"inputs.shape:\",  inputs.shape)\n",
    "    print(\"inputs.dtype:\",  inputs.dtype)\n",
    "    print(\"targets.dtype:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\",     inputs[0])\n",
    "    print(\"targets[0]:\",    targets[0])\n",
    "    break\n",
    "\n",
    "  return binary_3gram_train_ds, binary_3gram_test_ds, binary_3gram_val_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get_model_fn()\n",
    "get_model_fn( max_tokens=20000, hidden_dim=16)\n",
    "\n",
    "Receive the max number of tokes and the number of layers.\n",
    "Use those parameters to create a dense model of the given dimension.\n",
    "The model's activation is relu      \n",
    "      \n",
    "\"\"\"\n",
    "def get_model_fn( max_tokens=20000, hidden_dim=16) :\n",
    "  inputs = keras.Input(shape=(max_tokens,))\n",
    "  x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "  x = layers.Dropout(0.5)(x)\n",
    "  outputs = layers.Dense(1, activation=\"sigmoid\") (x)\n",
    "  model = keras.Model(inputs, outputs)\n",
    "  model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "  return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1741393817.142522   53104 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9411 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:08:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b\"I remember this film, exhibit in Barcelona (Spain) in 1970, for the time of a week. Although it could seems incredible, and I can't offer any explanation for it, this movie was exhibit in a theater dedicated to... movies of art and big quality (that, is, Bergman, Resnais, Malle, Bu\\xc3\\xb1uel, and... The Projected Man). Few people saw it (luckly people, no doubt) and no reference about this very boring SF movie can be found in the Peter Nichols Science Fiction Encyclopidie, or about the author of the original novel. Very indicative. I remember of it, after all this years, a no-story, a lot of special effects that seems ridiculous effects in fact, and no more. It seems that in some countries the running time is 90 mm. and in anothers 77 min. Well, it means only a little more of pain.\", shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 19:30:29.215741: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'int64'>\n",
      "targets.dtype: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1 1 1 ... 0 0 0], shape=(20000,), dtype=int64)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">320,016</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20000\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │       \u001b[38;5;34m320,016\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741393840.116030   53270 service.cc:148] XLA service 0x72adb00519d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1741393840.116067   53270 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2025-03-07 19:30:40.132655: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1741393840.193392   53270 cuda_dnn.cc:529] Loaded cuDNN version 90501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 30/625\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5887 - loss: 0.6675  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1741393840.561832   53270 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7848 - loss: 0.4660 - val_accuracy: 0.9086 - val_loss: 0.2552\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9129 - loss: 0.2468 - val_accuracy: 0.9114 - val_loss: 0.2463\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9308 - loss: 0.2111 - val_accuracy: 0.9124 - val_loss: 0.2554\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9427 - loss: 0.1877 - val_accuracy: 0.9046 - val_loss: 0.2762\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9495 - loss: 0.1725 - val_accuracy: 0.9078 - val_loss: 0.2870\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9494 - loss: 0.1786 - val_accuracy: 0.9038 - val_loss: 0.3112\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9533 - loss: 0.1664 - val_accuracy: 0.8994 - val_loss: 0.3172\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9540 - loss: 0.1639 - val_accuracy: 0.9038 - val_loss: 0.3374\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9559 - loss: 0.1511 - val_accuracy: 0.8952 - val_loss: 0.3520\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9571 - loss: 0.1460 - val_accuracy: 0.8972 - val_loss: 0.3625\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8939 - loss: 0.2764\n",
      "Test acc: 0.898\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "      jev_main_fn()\n",
    "\n",
    "Start the execution of the code in this file\n",
    "\n",
    "\"\"\"\n",
    "def jev_main_fn() :\n",
    "  strBaseDir  = \"/drv3/hm3/Data/IMDB/aclImdb\"\n",
    "\n",
    "  # call create_validation_set_fn() once to create 5K files in two directories (pos, neg)\n",
    "  # each with 2.5 k files. The 5K files are taken away from the train set, therefore the\n",
    "  # size of the training set will be 25k - 5k = 20K \n",
    "  #\n",
    "  #  create_validation_set_fn() \n",
    "  #\n",
    "\n",
    "  train_ds, test_ds, val_ds = create_data_sets_fn( strBaseDir )\n",
    "  binary_3gram_train_ds, binary_3gram_test_ds, binary_3gram_val_ds = textvectorization_preprocessing_fn( train_ds, test_ds, val_ds )\n",
    "\n",
    "  model = get_model_fn()\n",
    "  model.summary()\n",
    "  callbacks = [ keras.callbacks.ModelCheckpoint(\"binary_3gram.keras\", save_best_only=True) ]\n",
    "\n",
    "  model.fit(  binary_3gram_train_ds.cache(), \n",
    "              validation_data= binary_3gram_val_ds.cache(),\n",
    "              epochs=10, callbacks=callbacks)\n",
    "  model=keras.models.load_model(\"binary_3gram.keras\")\n",
    "  print(f\"Test acc: {model.evaluate(binary_3gram_test_ds)[1]:.3f}\")\n",
    "\n",
    "  print(\"Done\")\n",
    "\n",
    "#### LET\"S GO ! ! ! ! \n",
    "jev_main_fn()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
