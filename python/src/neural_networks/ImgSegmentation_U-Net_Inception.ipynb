{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "FileName = ImgSegmentation_U-Net_Inception.ipynb. The code here shows how to use the U-Net Architecture for Semantic Img Segmentation using TF/Keras  \n",
    "\n",
    "The dataset used comes from he Oxford IIT Pets data set. The code is located at\n",
    "\n",
    "    /drv3/hm3/code/python/keras.3.02/\n",
    "\n",
    "The original code was running in a Virtual Env under Python 3.10.12, with TF/Keras 2.15\n",
    "The data is supposed to be obtain as\n",
    "\n",
    "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
    "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
    "!tar -xf images.tar.gz\n",
    "!tar -xf annotations.tar.gz\n",
    "\n",
    "But instead I visited Kaggle, obtained the dataset and stored it at \n",
    "\n",
    "/dev3/hm3/Data/ImgData/Oxford_iit_pets\n",
    "\n",
    "As of Feb 2025, the code is running under TF/Keras 2.18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageOps\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# APIs to plot a keras model using keras.utils.plot_model()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# besides installing graphviz via pip install graphviz,\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# I also had to install in Ubuntu via sudo apt install graphviz \u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydot\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydot'"
     ]
    }
   ],
   "source": [
    "# get the libraries ready\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import shutil, pathlib\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image \n",
    "from PIL import ImageOps\n",
    "\n",
    "# APIs to plot a keras model using keras.utils.plot_model()\n",
    "# besides installing graphviz via pip install graphviz,\n",
    "# I also had to install in Ubuntu via sudo apt install graphviz \n",
    "import pydot\n",
    "import graphviz\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow import image as tf_image\n",
    "from tensorflow import io as tf_io\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.utils import load_img, img_to_array\n",
    "from keras.utils import array_to_img\n",
    "\n",
    "print(\"pydot version    = \", pydot.__version__)\n",
    "print(\"graphviz version = \", graphviz.__version__)\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "print(matplotlib.__version__)\n",
    "print( tf.config.list_physical_devices('GPU') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "base_dat_dir = \"/drv3/hm3/Data/ImgData/Oxford_iiit_pets/\"\n",
    "\n",
    "input_dir  = base_dat_dir + \"images/\"   # contains the 7,394 images\n",
    "target_dir = base_dat_dir +  \"annotations/trimaps/\"\n",
    "\n",
    "# there are 7390 files in input_img_paths\n",
    "input_paths = sorted(\n",
    "    [os.path.join(input_dir, fname)\n",
    "     for fname in os.listdir(input_dir)\n",
    "     if fname.endswith(\".jpg\")])\n",
    "\n",
    "# there are 7390 files in target_img_paths\n",
    "target_paths = sorted(\n",
    "    [os.path.join(target_dir, fname)\n",
    "     for fname in os.listdir(target_dir)\n",
    "     if fname.endswith(\".png\") and not fname.startswith(\".\")])\n",
    "\n",
    "img_size = (160, 160)\n",
    "num_classes = 3\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# image displayed is \"Abyssinian_107.jpg\" Image is of size 151 KBs\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(load_img(input_paths[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def display_target(target_array):\n",
    "    normalized_array = (target_array.astype(\"uint8\") - 1) * 127\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(normalized_array[:, :, 0])\n",
    "\n",
    "img = img_to_array(load_img(target_paths[9], color_mode=\"grayscale\"))\n",
    "display_target(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "The images in the dataset have different shapes and sizes.\n",
    "Images and mask must be of the same shape (size).\n",
    "Returns a TF Dataset, which consists of a tuple with ( inputFiles, TargetFiles) \n",
    "The function is used to create the dataset for train and for validation   \n",
    "\"\"\"\n",
    "def get_dataset( batch_size, img_size, input_img_paths, target_img_paths, max_dataset_len=None) :\n",
    "\n",
    "    def load_img_masks(input_img_path, target_img_path):\n",
    "        input_img = tf_io.read_file(input_img_path)\n",
    "        input_img = tf_io.decode_png(input_img, channels=3)\n",
    "        input_img = tf_image.resize(input_img, img_size)\n",
    "        input_img = tf_image.convert_image_dtype(input_img, \"float32\")\n",
    "\n",
    "        target_img = tf_io.read_file(target_img_path)\n",
    "        target_img = tf_io.decode_png(target_img, channels=1)\n",
    "        target_img = tf_image.resize(target_img, img_size, method=\"nearest\")\n",
    "        target_img = tf_image.convert_image_dtype(target_img, \"uint8\")\n",
    "\n",
    "        # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "        target_img -= 1\n",
    "        return input_img, target_img\n",
    "\n",
    "    # For faster debugging, limit the size of data\n",
    "    if max_dataset_len:\n",
    "        input_paths = input_paths[:max_dataset_len]\n",
    "        target_paths = target_paths[:max_dataset_len]\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((input_img_paths, target_img_paths))\n",
    "    dataset = dataset.map(load_img_masks, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "# Split our img paths into training and validation sets\n",
    "val_samples = 1000\n",
    "random.Random(1337).shuffle(input_paths)\n",
    "random.Random(1337).shuffle(target_paths)\n",
    "\n",
    "train_input_paths = input_paths[:-val_samples]\n",
    "train_target_paths = target_paths[:-val_samples]\n",
    "\n",
    "val_input_paths = input_paths[-val_samples:]\n",
    "val_target_paths = target_paths[-val_samples:]\n",
    "\n",
    "# Instantiate dataset for each split\n",
    "# Limit input files in `max_dataset_len` for faster epoch training time.\n",
    "# Remove the `max_dataset_len` arg when running with full dataset.\n",
    "train_dataset = get_dataset(batch_size, img_size, train_input_paths, train_target_paths,\n",
    "    # max_dataset_len=1000,\n",
    ")\n",
    "val_dataset = get_dataset( batch_size, img_size, val_input_paths, val_target_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data is ready\n",
    "\n",
    "target_img_file = train_target_paths[0]  \n",
    "target_img = mpimg.imread( target_img_file)\n",
    "print(\"target img shape = \" , target_img.shape )\n",
    "\n",
    "target_im_frame = Image.open( target_img_file)\n",
    "np_frame = np.array( target_im_frame.getdata())\n",
    "print(\"np shape = \", np_frame.shape)\n",
    "np2_frame = np.reshape( np_frame, np_frame.shape) \n",
    "print(\"np2_frame.shape = \", np2_frame.shape )\n",
    "#plt.imshow( target_img )\n",
    "\n",
    "input_img_file = train_input_paths[0]  \n",
    "input_img = mpimg.imread( input_img_file)\n",
    "print(\"input img shape = \" , input_img.shape )\n",
    "\n",
    "input_im_frame = Image.open( input_img_file)\n",
    "np_frame = np.array( input_im_frame.getdata())\n",
    "print(\"np shape = \", np_frame.shape)\n",
    "np2_frame = np.reshape( np_frame, np_frame.shape) \n",
    "print(\"np2_frame.shape = \", np2_frame.shape )\n",
    "#plt.imshow(input_img )\n",
    "\n",
    "# Initialise the subplot function using number of rows and columns\n",
    "figure, axis = plt.subplots(1, 2)\n",
    "  \n",
    "# Plot input Img \n",
    "axis[0].imshow(input_img)\n",
    "axis[0].set_title(\"Input Img\")\n",
    "  \n",
    "# For target Img \n",
    "axis[1].imshow(target_img)\n",
    "axis[1].set_title(\"Target Img\")\n",
    "\n",
    "# Combine imgs in a tight layout  \n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# define the u-net-inception \n",
    "\n",
    "def get_model(img_size, num_classes):\n",
    "    inputs = keras.Input(shape=img_size + (3,))\n",
    "\n",
    "    ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Add a per-pixel classification layer\n",
    "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = get_model(img_size, num_classes)\n",
    "model.summary()\n",
    "# keras.utils.plot_model( model, to_file=\"/drv3/hm3/code/python/keras.3.02/u-net-inception-model.png\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data with the u-net-inception model  \n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam( 1e-4), loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=\"accuracy\")\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "callback_dir = \"/drv3/hm3/code/python/tf2.18/tf2.18/local/ImgSegmentation/Playground/\"\n",
    "\n",
    "callbacks = [ keras.callbacks.ModelCheckpoint(callback_dir, save_best_only=True) ]\n",
    "\n",
    "history = model.fit(train_dataset, validation_data= val_dataset, verbose=0, epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history[\"loss\"]) + 1)\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# val_dataset is already created\n",
    "\n",
    "\"\"\"\n",
    "To visualize the predictions that the model makes, do the following:\n",
    "\n",
    "1. Use keras.models.load_model( callbackdir ) to get the model's params\n",
    "\n",
    "2. Select target imgs from val_dataset (those imgs have not been seen by the model before).\n",
    "\n",
    "3. Several variables are used to keep track of where things are:\n",
    "\n",
    "    it is a list of .jpg file names\n",
    "    it is a list of PIL images\n",
    "    it is a list of np.array objs, which were converted from the PIL images\n",
    " \n",
    "    pil image\n",
    "    np.array\n",
    "\n",
    "3. The input files are .jpg files which can be can be displayed directly with code like\n",
    "   test_image = val_input_paths[i]\n",
    "   plt.imshow(load_img(test_image))\n",
    "\n",
    "4. To do prediction call Model.predict(x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None)\n",
    "    X could be (1) np.array, (2) list of np.arrays, (3) Tensor, (4) List of Tensors, \n",
    "    (5) tf.data.Dataset, (6) keras.utils.Pydataset\n",
    "5. If type(X) == np.array , X must be converted from PIL to an np.array, using somethihg like this:\n",
    "\n",
    "    val_target_file = train_target_paths[0]  \n",
    "    val_target_img = mpimg.imread( val_target_file)\n",
    "    print(\"val target img shape = \" , val_target_img.shape )\n",
    "\n",
    "    target_im_frame = Image.open( target_img_file)\n",
    "    np_frame = np.array( target_im_frame.getdata())\n",
    "    print(\"np shape = \", np_frame.shape)\n",
    "    np2_frame = np.reshape( np_frame, np_frame.shape) \n",
    "    print(\"np2_frame.shape = \", np2_frame.shape )\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# model = keras.models.load_model(callbacks)\n",
    "model = keras.models.load_model(callback_dir)\n",
    "\n",
    "val_preds = model.predict(val_dataset) # Shape is (1000, 160, 160, 3)\n",
    "print(\"val_preds.shape =\", val_preds.shape)\n",
    "\n",
    "print(\"val_preds[1].shape =\", val_preds[1].shape)\n",
    "\n",
    "\n",
    "# img to validate\n",
    "i = 10\n",
    "\n",
    "test_image = val_input_paths[i]\n",
    "plt.imshow(load_img(test_image))\n",
    "\n",
    "val_target_file = val_target_paths[0]  \n",
    "val_target_img = mpimg.imread( val_target_file )\n",
    "print(\"val target img shape = \" , val_target_img.shape )\n",
    "\n",
    "target_im_frame = Image.open( val_target_file)\n",
    "np_frame = np.array( target_im_frame.getdata())\n",
    "print(\"np shape = \", np_frame.shape)\n",
    "np2_frame = np.reshape( np_frame, np_frame.shape) \n",
    "print(\"np2_frame.shape = \", np2_frame.shape )\n",
    "\n",
    "# mask = model.predict( np.expand_dims(val_target_img[i],0 ))\n",
    "# mask = model.predict( np_frame )     \n",
    "# mask = model.predict(np2_frame)\n",
    "mask = model.predict(np.expand_dims(test_image, 0))[0]\n",
    "\n",
    "\n",
    "# plt.imshow(array_to_img(test_image))\n",
    "\n",
    "# mpimg.imread( test_image)\n",
    "\n",
    "# mask = model.predict(np.expand_dims(test_image, 0))[0]\n",
    "\n",
    "# Display results for validation image #10\n",
    "\n",
    "\n",
    "# Initialise the subplot function using number of rows and columns\n",
    "#figure, axis = plt.subplots(1, 3)\n",
    "\n",
    "#val_input_file = val_input_paths[i]  \n",
    "#plt.imshow(test_image)\n",
    "# plt.imshow(array_to_img(test_image))\n",
    "\n",
    "#val_input_img = mpimg.imread( val_input_file)\n",
    "\n",
    "# Plot input Img \n",
    "#axis[0].imshow(val_input_img)\n",
    "#axis[0].set_title(\"Val Img\")\n",
    "\n",
    "#val_target_file = val_target_paths[i]\n",
    "#val_target_img = mpimg.imread( val_target_file)\n",
    "#axis[1].imshow(val_target_img)\n",
    "#axis[1].set_title(\"Val Mask \")\n",
    "\n",
    "#val_target_file = val_target_paths[i]\n",
    "#val_target_img = mpimg.imread( val_target_file)\n",
    "#axis[2].imshow(val_target_img)\n",
    "#axis[2].set_title(\"Predicted Mask \")\n",
    "\n",
    "\n",
    "# For target Img \n",
    "#axis[1].imshow(target_img)\n",
    "#axis[1].set_title(\"Target Img\")c\n",
    "\n",
    "# Combine imgs in a tight layout  \n",
    "#plt.tight_layout() \n",
    "#plt.show()\n",
    "\n",
    "\"\"\"\n",
    "model = keras.models.load_model(callback_dir)\n",
    "\n",
    "i = 4\n",
    "test_image = val_input_imgs[i]\n",
    "plt.axis(\"off\")c\n",
    "plt.imshow(array_to_img(test_image))\n",
    "\n",
    "mask = model.predict(np.expand_dims(test_image, 0))[0]\n",
    "\n",
    "def display_mask(pred):\n",
    "    mask = np.argmax(pred, axis=-1)\n",
    "    mask *= 127\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(mask)\n",
    "\n",
    "display_mask(mask)\n",
    "\n",
    "   \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset already created. N oite the args to the call \n",
    "# val_dataset = get_dataset( batch_size, img_size, val_input_paths, val_target_paths)\n",
    "\n",
    "val_dataset = get_dataset(\n",
    "    batch_size, img_size, val_input_paths, val_target_paths\n",
    ")\n",
    "val_preds = model.predict(val_dataset)\n",
    "\n",
    "\n",
    "def display_mask(i):\n",
    "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
    "    mask = np.argmax(val_preds[i], axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    img = ImageOps.autocontrast(keras.utils.array_to_img(mask))\n",
    "    display(img)\n",
    "\n",
    "\n",
    "# Display results for validation image #10\n",
    "i = 10\n",
    "\n",
    "# Display target image\n",
    "lofas_img = mpimg.imread( val_target_paths[i] )\n",
    "plt.imshow(lofas_img)\n",
    "\n",
    "# Display target image\n",
    "lofas_img = mpimg.imread( val_input_paths[i] )\n",
    "plt.imshow(lofas_img)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter09_part01_image-segmentation.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tf2.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
