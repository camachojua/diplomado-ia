{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   imdb_dense_torch_001.ipynb\n",
    "\n",
    "1. Run under a PyTorch virtual env\n",
    "\n",
    "2. Get the imdb_data from the files created in imdb_dense_torch_0000.ipynb\n",
    "\n",
    "3. Modify the data structures as necessary\n",
    "\n",
    "4. Use the data to test a PyTorch Dense model with parameters similar to the ones we used under TF/Keras\n",
    "\n",
    "5. Compare results vis-a vis the results using TF Dense Models \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mimic the IMDB Data, which is typically set as list of (review, label) tuples.\n",
    "# The reviews are lists of word indices, from 1...10000, going from more important to less important\n",
    "# The labels are 0 or 1, indicating a bad or good review, respectively.\n",
    "\n",
    "# I created the data manually, to ensure that the data preprocessing, the models etc, work well.\n",
    "# There are only three different lists in this data to ensure that models can learn and predict\n",
    "# with high accuracy. Results with low accuracy will reveal problems in the data pre-processing,\n",
    "# the models configurations, or both...\n",
    "\n",
    "imdb_fake_data = [\n",
    "    ([1, 2, 3, 4, 5], 1),  # Review: [word1, word2, ...], Label: 1\n",
    "    ([6, 7, 8], 0),        # Review: [word1, word2, ...], Label: 0\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([6, 7, 8], 0),\n",
    "    ([6, 7, 8], 0),\n",
    "    ([6, 7, 8], 0),\n",
    "    ([6, 7, 8], 0),\n",
    "    ([6, 7, 8], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    # ... more data\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig fake data\n",
      "[([1, 2, 3, 4, 5], 1), ([6, 7, 8], 0), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([6, 7, 8], 0), ([6, 7, 8], 0), ([6, 7, 8], 0), ([6, 7, 8], 0), ([6, 7, 8], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0)]\n",
      "fake data from disk\n",
      "[([1, 2, 3, 4, 5], 1), ([6, 7, 8], 0), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([6, 7, 8], 0), ([6, 7, 8], 0), ([6, 7, 8], 0), ([6, 7, 8], 0), ([6, 7, 8], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0)]\n",
      "TF ragged data from disk\n",
      "[list([1, 14, 22, 16, 43, 530, 32])\n",
      " list([1, 194, 1153, 194, 8255, 78, 95])\n",
      " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 7, 129, 13])]\n",
      "lofas\n",
      "[1, 14, 22, 16, 43, 530, 32]\n",
      "[1, 194, 1153, 194, 8255, 78, 95]\n",
      "[1, 14, 47, 8, 30, 31, 7, 4, 249, 7, 129, 13]\n"
     ]
    }
   ],
   "source": [
    "# Use pickle to write/read imdb_fake data to/from disk\n",
    "# import pickle\n",
    "\n",
    "# Save to disk\n",
    "with open(\"/drv3/hm3/tmp/imdb_fake_data.pkl\", \"wb\") as f:\n",
    "   pickle.dump(imdb_fake_data, f)\n",
    "\n",
    "# Load from disk\n",
    "with open(\"/drv3/hm3/tmp/imdb_fake_data.pkl\", \"rb\") as f:\n",
    "   imdb_fake_ragged_data = pickle.load(f)\n",
    "\n",
    "print(\"Orig fake data\")\n",
    "print(imdb_fake_data)\n",
    "\n",
    "print(\"fake data from disk\")\n",
    "print(imdb_fake_ragged_data)   \n",
    "\n",
    "with open(\"/drv3/hm3/tmp/ragged_array.pkl\", \"rb\") as f:\n",
    "   tf_ragged_data = pickle.load(f)\n",
    "\n",
    "print(\"TF ragged data from disk\")\n",
    "print(tf_ragged_data)\n",
    "\n",
    "print(\"lofas\" )\n",
    "for i in tf_ragged_data :\n",
    "   print( i )\n",
    "\n",
    "\n",
    "with open(\"/drv3/hm3/tmp/train_data_as_ragged_array.pkl\", \"rb\") as f:\n",
    "   tf_train_data_as_ragged_array = pickle.load(f)\n",
    "\n",
    "with open(\"/drv3/hm3/tmp/train_labels_as_ragged_array.pkl\", \"rb\") as f:\n",
    "   tf_train_labels_as_ragged_array = pickle.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewData:  # Custom class to control printing\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data, dtype=object)\n",
    "\n",
    "    def __str__(self):  # Override the string representation\n",
    "        return \"[\" + \" \".join(str(tuple(item)) for item in self.data) + \"]\"\n",
    "\n",
    "    def __getitem__(self, index):  # Allow indexing\n",
    "        return self.data[index]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32], 1)\n",
      "i\n",
      "([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95], 0)\n",
      "i\n",
      "([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113], 0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf_train_data  = tf_train_data_as_ragged_array\n",
    "tf_train_labels = tf_train_labels_as_ragged_array \n",
    "tf_train_labels = tf_train_labels.astype(object)\n",
    "\n",
    "nRows = 25000\n",
    "nCols = 2\n",
    "imdb_pt_data = np.empty(0, dtype=object)\n",
    "imdb_pt_data.resize(nRows)\n",
    "\n",
    "# note: since the revies are long, placed the labels first when I was verifying that the data is being set up correctly\n",
    "for i in range(nRows):\n",
    "  d = tf_train_data[i]\n",
    "  l = tf_train_labels[i]\n",
    "  #print(\"label[\",i, \"] = \", l)\n",
    "  #print(\"data [\",i, \"] = \", d )\n",
    "  ll = (d,l)\n",
    "  imdb_pt_data[i] = ll\n",
    "  #print(\"ll = \", ll )\n",
    "  #print(\"\")\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "  print(\"i\")\n",
    "  print( imdb_pt_data[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Loading with Padding\n",
    "def collate_fn(batch):\n",
    "    reviews, labels = zip(*batch)\n",
    "    review_lengths = torch.tensor([len(r) for r in reviews])  # Store original lengths\n",
    "    padded_reviews = pad_sequence(reviews, batch_first=True, padding_value=0) # Pad to max length in batch\n",
    "    labels = torch.stack(labels) # Stack labels\n",
    "    return padded_reviews, labels, review_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Custom Dataset Class\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review, label = self.data[idx]\n",
    "        return torch.tensor(review), torch.tensor(label)  # Convert to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am using data from imdb_pt_data. Note that the array has a few data poits (25) because \n",
    "# I am testing if the data transformation from tf to pt was done OK\n",
    "\n",
    "# Split training and test data. Training gets 80% of the data\n",
    "\n",
    "# \n",
    "\n",
    "# train_data, test_data = train_test_split(imdb_fake_data, test_size=0.2, random_state=42)\n",
    "train_data, test_data = train_test_split(imdb_pt_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = IMDBDataset(train_data)\n",
    "test_dataset = IMDBDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " #4. Define a Pytorch Dense Model\n",
    "\n",
    "class DenseModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super(DenseModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)  # Word embeddings\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # Create mask *before* embedding. 1 for non-padding, 0 for padding\n",
    "        mask = x != 0  # (batch_size, seq_len) - True for non-padding, False for padding\n",
    "        masked_embedded = embedded * mask.unsqueeze(-1).float() # Apply mask to embeddings\n",
    "\n",
    "        # Average pooling over sequence length (handles variable lengths)\n",
    "        lengths = lengths.unsqueeze(1).float()  # (batch_size, 1)\n",
    "        pooled = masked_embedded.sum(dim=1) / lengths  # Average pool\n",
    "\n",
    "        x = self.fc1(pooled)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Training and Validation Loops\n",
    "input_dim = 10000  # Adjust size of vocabulary to 10000 \n",
    "embedding_dim = 128  # Size of word embeddings\n",
    "hidden_dim = 256\n",
    "output_dim = 2  # Binary classification (0 or 1)\n",
    "\n",
    "model = DenseModel(input_dim, embedding_dim, hidden_dim, output_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # use gpu if available\n",
    "model.to(device) # move model to gpu\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.3896, Accuracy: 85.80%\n",
      "Epoch [2/20], Loss: 0.3711, Accuracy: 87.82%\n",
      "Epoch [3/20], Loss: 0.2755, Accuracy: 87.76%\n",
      "Epoch [4/20], Loss: 0.0708, Accuracy: 87.76%\n",
      "Epoch [5/20], Loss: 0.1111, Accuracy: 87.20%\n",
      "Epoch [6/20], Loss: 0.0595, Accuracy: 87.56%\n",
      "Epoch [7/20], Loss: 0.0524, Accuracy: 87.12%\n",
      "Epoch [8/20], Loss: 0.0375, Accuracy: 86.30%\n",
      "Epoch [9/20], Loss: 0.0448, Accuracy: 87.02%\n",
      "Epoch [10/20], Loss: 0.0269, Accuracy: 86.82%\n",
      "Epoch [11/20], Loss: 0.0113, Accuracy: 86.56%\n",
      "Epoch [12/20], Loss: 0.0041, Accuracy: 86.40%\n",
      "Epoch [13/20], Loss: 0.0124, Accuracy: 86.20%\n",
      "Epoch [14/20], Loss: 0.0024, Accuracy: 86.32%\n",
      "Epoch [15/20], Loss: 0.0011, Accuracy: 86.46%\n",
      "Epoch [16/20], Loss: 0.0030, Accuracy: 85.98%\n",
      "Epoch [17/20], Loss: 0.0008, Accuracy: 86.14%\n",
      "Epoch [18/20], Loss: 0.0002, Accuracy: 86.10%\n",
      "Epoch [19/20], Loss: 0.0000, Accuracy: 86.38%\n",
      "Epoch [20/20], Loss: 0.0001, Accuracy: 86.32%\n"
     ]
    }
   ],
   "source": [
    "# change number of epochs as necessary with the real IMDB data.\n",
    "#  Maybe just 10 will be enough \n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()  # Set model to training mode\n",
    "    for padded_reviews, labels, lengths in train_loader:\n",
    "      padded_reviews = padded_reviews.to(device) # move data to gpu\n",
    "      labels = labels.to(device) # move data to gpu\n",
    "      lengths = lengths.to(device) # move data to gpu\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(padded_reviews, lengths)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradients during validation\n",
    "        for padded_reviews, labels, lengths in test_loader:\n",
    "          padded_reviews = padded_reviews.to(device) # move data to gpu\n",
    "          labels = labels.to(device) # move data to gpu\n",
    "          lengths = lengths.to(device) # move data to gpu\n",
    "          outputs = model(padded_reviews, lengths)\n",
    "          _, predicted = torch.max(outputs.data, 1) # get the prediction\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item() # count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.32%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.86      2437\n",
      "           1       0.86      0.87      0.87      2563\n",
      "\n",
      "    accuracy                           0.86      5000\n",
      "   macro avg       0.86      0.86      0.86      5000\n",
      "weighted avg       0.86      0.86      0.86      5000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2080  357]\n",
      " [ 327 2236]]\n"
     ]
    }
   ],
   "source": [
    "# 6. Evaluation (After Training) - More Detailed\n",
    "model.eval()  # Set to evaluation mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # use gpu if available\n",
    "model.to(device) # move model to gpu\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for padded_reviews, labels, lengths in test_loader:\n",
    "        padded_reviews = padded_reviews.to(device) # move data to gpu\n",
    "        labels = labels.to(device) # move data to gpu\n",
    "        lengths = lengths.to(device) # move data to gpu\n",
    "        outputs = model(padded_reviews, lengths)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())  # Store predictions for later analysis\n",
    "        all_labels.extend(labels.cpu().numpy())      # Store true labels\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# 7. Additional Evaluation Metrics (using scikit-learn)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_predictions))\n",
    "\n",
    "\n",
    "# Example of getting predictions for a single review\n",
    "def predict(review_indices):\n",
    "    model.eval()\n",
    "    review_tensor = torch.tensor([review_indices]).to(device)  # Add batch dimension\n",
    "    review_length = torch.tensor([len(review_indices)]).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(review_tensor, review_length)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    return predicted.item()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
